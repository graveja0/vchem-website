---
title: "On the Discrete Choice of Health Care Providers"
subtitle: "Part 1: The Equivalence of Utility Maximization and Gravity Model Frameworks"
author: John Graves
date: "2023-01-16"
categories: [Health Economics]
editor_options: 
  chunk_output_type: console
execute:
  echo: true
  message: false
  warning: false  
  code-fold: true
  freeze: auto 
bibliography: "references.bib"
reference-location: margin
self-contained: true
image: "media/DALY-image.svg"
editor: 
  markdown: 
    wrap: 72
---

Suppose you are a patient requiring treatment from a hospital. Suppose further that you have three nearby hospitals (A, B and C)‌ to choose among. What considerations do you take into account when making your treatment choice?

Modeling consumer choice has a long history in economics. The dominant approach remains anchored in utilitarian welfare maximization theory. Beginning with @mcfadden1973conditional, economists matched utility maximization-based micro-foundations with the econometric/statistical concepts of maximum likelihood to yield a tractable approach to estimating discrete choice models. These models, including the multinomial logit other valuable extensions, have served as the bedrock estimation approach for consumer and discrete choice for a generation+ of economists. 

Medical geography and health services research, on the other hand, have approached the provider choice question from a different perspective. In those literatures, another popular method conceptualizes choice as the outcome from patients weighing the “attractiveness“‌ of different options. In the context of hospital choice, the relative attractiveness of one facility over another is based on patients weighing their own attributes (e.g., age, acuity, etc.), as well as those of the hospital (e.g., quality), against other ”frictions“‌ (e.g., travel time or distance)‌ that influence their choices. 

While these two approaches share some commonalities (e.g., the use of patient and hospital attributes, as well as distance, as key features of the underlying choice problem) they are also conceptually different. Discrete choice theory and its estimation assumptions/methods are rooted in micro-economic principles, while so-called “gravity models” are adapted from physics—specifically, a re-conceptualization of Newton’s Law of Gravitation, which states that the force between two objects (patients and hospitals) is directly proportional to their mass (i.e., their “attractiveness”) and inversely proportional to the distance between them.[^trade] 

[^trade]: Gravity models do have a noteworthy lineage within economics, as well. However, their use has flourished primarily in international trade and not in consumer/discrete choice theory. 

In this post, we resurrect a forty-year-old paper in transportation research (@anas1983) that demonstrates the equivalence between the multinomial logit and a gravity model. In other words, for a given set of patient and provider attributes, a multinomial logit and a gravity model will yield identical predicted choice probabilities. 

Bridging these two approaches proves useful because it provides a nice foundation for thinking about how to specify a discrete choice problem. In a simulated example, we’ll consider the choice of hospital in the context of privately insured patients who face higher cost-sharing if they are treated (in a nonemergency)‌ at an out-of-network facility. Conceptually, the patient’s plan-level attributes will contribute as an additional source of “friction” on hospital choice. In other words, all else equal, a patient may prefer a nearby facility that is of high (perceived)‌ quality. But if that facility is out-of-network, the patient may not even consider it for treatment; it may as well be 1,000 miles away. 




<!-- https://cran.r-project.org/web/packages/gravity/vignettes/crash-course-on-gravity-models.html -->
<!-- https://www.sciencedirect.com/science/article/abs/pii/B9780444543141000033?via%3Dihub DOUBLE DEMEANING-->
<!-- https://journals.sagepub.com/doi/pdf/10.1177/1536867X1501500210 -->


# Information Theory and Discrete Choice

An alternative way of conceptualizing a choice problem is through information and decision theory; this approach will eventually serve as the foundation for a gravity model of discrete choice later. 

Information theory grew out of seminal work by Claude Shannon in 1948, and is arguably one of the most important intellectual contributions in the 20th century, if not all of human history [@shannon1948mathematical]. Information theory serves as the backbone for modern digital communication, guiding how data are stored and transmitted across the internet, cell phones, and satellite systems. It is also used widely in artificial intelligence and machine learning. 

A key concept of information theory is the idea of entropy, or the degree of uncertainty or randomness in a set of data. Shannon’s measure of entropy quantifies the amount of information in a message, data source, or (for our purposes here)  choice among alternatives. 

In the context of the choice over three hospitals, our objective is to account for features or attributes of the choice problem that provide information on the decision. Then, conditional on this information, we want to find the choice probabilities that maximize entropy or uncertainty in the decisions. 

![Decision Tree for Entropy Example](media/decision-tree.png){#fig-decisiontree fig-align="center" width="50%"}


```{r, eval = FALSE}
#| code-fold: true
#| code-summary: Reproduce the entropy example
#| 
# Entropy example for figure
set.seed(123)
X <- paste0(sample(c("A","B","C"),size=20,replace=TRUE,prob=c(.05,.15,.8)),collapse="")
X <- paste0(X,paste0(sample(c("A","B","C"),size=20,replace=TRUE,prob=c(.8,.15,.05)),collapse=""),collapse="")
X_ <- table(unlist(strsplit(X,"")))/40; X_
-sum(X_*log(X_,2))

x <- substr(X,1,20)
x_ <- table(unlist(strsplit(x,"")))/20
-sum(x_*log(x_,2))

x <- substr(X,21,40)
x_ <- table(unlist(strsplit(x,"")))/20
-sum(x_*log(x_,2))

```



# Gravity Model

$$
\underset{\left[P_j^h\right]}{\operatorname{Minimize}}-\mathscr{E}=\sum_h \sum_j P_j^h \log P_j^h
$$
subject to:
$$
\begin{gathered}
\sum_j P_j^h=1 ; \quad h=1 \ldots H \\
\sum_h P_j^h=\sum_h \delta_j^h ; \quad j=1 \ldots J \\
\sum_h \sum_j P_j^h X_{j k}^h=\sum_h \sum_j \delta_j^h X_{j k}^h ; \quad k=1 \ldots K .
\end{gathered}
$$

```{r}
needed_packages <- c("tidyverse","glue","progress","here","evd","geosphere","mlogit","dfidx","janitor",
                     "Formula","hrbrthemes","survival","furrr","progressr","tictoc","mcreplicate","haven",
                     "fastDummies")

have <- rownames(installed.packages())
needed <- setdiff(needed_packages, have)

please_install <- function(pkgs, install_fun = install.packages) {

    if (length(pkgs) == 0) {
        return(invisible())
    }
    if (!interactive()) {
        stop("Please run in interactive session", call. = FALSE)
    }

    title <- paste0(
        "Ok to install these packges?\n",
        paste("* ", pkgs, collapse = "\n")
    )
    ok <- menu(c("Yes", "No"), title = title) == 1

    if (!ok) {
        return(invisible())
    }

    install_fun(pkgs,dependencies=TRUE)
}
please_install(needed)
lapply(needed_packages, require, character.only = TRUE)

options("scipen" = 100, "digits" = 5)
select <- dplyr::select

map_progress <- function(.x, .f, ..., .id = NULL) {
    # Source: https://www.jamesatkins.net/post/2020/progress-bar-in-purrrmap_df/
    .f <- purrr::as_mapper(.f, ...)
    pb <- progress::progress_bar$new(total = length(.x), force = TRUE)
    
    f <- function(...) {
        pb$tick()
        .f(...)
    }
    purrr::map(.x, f, ..., .id = .id)
}


map_multicore <- function(.x, .f, ..., .id = NULL) {
    .f <- purrr::as_mapper(.f, ...)
    p <- progressor(steps = length(.x))
    f <- function(...) {
        p()
        .f(...)
    }
    furrr::future_map(.x, f, ..., .id = .id)
}
```

# Data and Parameters

## HCUP Data

```{r, eval = FALSE}
# df <-
#     list.files("/Volumes/HPD/HCUP/Graves/sas_data_aha_linked/SID/") %>%
#     grep("^az_",.,value=TRUE) %>%
#     grep("core",.,value=TRUE) %>%
#     map(~(haven::read_sas(glue("/Volumes/HPD/HCUP/Graves/sas_data_aha_linked/SID/{.}"))))

# vars_to_include_ <- c("AHAID","ZIP","AGE","FEMALE","HCUP_ED","HCUP_OS","HISPANIC","KEY","PAY1","RACE")
# # vars_to_include <- vars_to_include_ %>% map_dbl(~(grep(glue("^{.x}$"),names(df[[1]]))))
# 
# df_sid <- 
#     list.files("/Volumes/HPD/HCUP/Graves/sas_data_aha_linked/SID/") %>% 
#     grep("^az_",.,value=TRUE) %>% 
#     grep("core",.,value=TRUE) %>% 
#     map(~(haven::read_sas(glue("/Volumes/HPD/HCUP/Graves/sas_data_aha_linked/SID/{.}"),col_select = vars_to_include_))) %>% 
#     pluck(1) %>% 
#     filter(!is.na(AHAID) & AHAID!="")
# 
# df_sid %>% write_rds("~/onedrive/Research-Arnold_Networks/data/hcup/blog-post/az-hcup.rds")

df_sid <- read_rds("~/onedrive/Research-Arnold_Networks/data/hcup/blog-post/az-hcup.rds")

hospital_ids <- 
    unique(df_sid$AHAID)

df_zip <- 
    read.csv(here("blog/posts/gravity/_data/ZIP-centroids_geocorr2018_2401102179.csv"),skip=1) %>% 
    as_tibble() %>% clean_names() %>% 
    rename(zip_x = wtd_centroid_w_longitude_degrees,
           zip_y = wtd_centroid_latitude_degrees) %>% 
    mutate(zip = str_pad(zip_census_tabulation_area,width=5,pad="0",side="left")) %>% 
    select(zip,zip_x,zip_y) %>% 
    filter(zip %in% unique(df_sid$ZIP))
```

## AHA Data
```{r, eval = FALSE}
# Load hospital location data.
aha_files <- c(
    "2019" = "/Users/johngraves/Library/CloudStorage/OneDrive-VUMC/Research-AHA_Data/data/aha/annual/raw/2019/2019_ASDB/COMMA/ASPUB19.CSV"
)

df_aha <-
    aha_files %>%
    map(~(
        data.table::fread(.x) %>%
            janitor::clean_names() %>%
            mutate(system_id = ifelse(!is.na(sysid),paste0("SYS_",sysid),id)) %>%
            dplyr::filter(serv==10))) %>%
    bind_rows() %>%
    as_tibble() %>%
    filter(id %in% hospital_ids) %>% 
    mutate(hosp_y = as.numeric(paste0(lat))) %>%
    mutate(hosp_x = as.numeric(paste0(long))) %>%
    select(id, hosp_x, hosp_y,mapp3,mapp8) %>%
    unique() %>%
    rename(hosp_id = id) %>% 
    mutate(teaching = case_when(mapp8==1 ~ 1,
                                mapp3==1 ~ 2,
                                .default=3)) %>% 
    mutate(teaching = factor(teaching,levels = 1:3, labels = c("major","minor","nonteaching"))) %>% 
    
    select(hosp_id,hosp_x,hosp_y,teaching) 
```

## Choice Sets

```{r, eval = FALSE}
dist_ <-
        crossing(zip = df_zip$zip ,hosp_id = df_aha$hosp_id) %>%
        as_tibble() %>%
        left_join(df_zip,"zip") %>%
        left_join(df_aha,"hosp_id") %>%
        rowwise() %>%
        mutate(dist = as.vector(distm(c(zip_x,zip_y),c(hosp_x,hosp_y), fun = distHaversine)*0.000621371)) %>%
        select(zip,hosp_id,dist) %>%
        set_names(c("zip","hosp_id","d_gh")) %>%
        data.frame() %>%
        dplyr::filter(d_gh < 200) %>%
        as_tibble()
```

```{r, eval = FALSE}
hosp_ <- 
    df_aha %>% 
    select(hosp_id, teaching)

df_choice <- 
    df_sid %>% 
    janitor::clean_names() %>% 
    mutate(key = paste0(key)) %>% 
    left_join(dist_,"zip",relationship = "many-to-many") %>% 
    left_join(hosp_, "hosp_id") %>% 
    mutate(choice = as.integer(ahaid==hosp_id)) %>% 
    group_by(key) %>% 
    filter(max(choice)==1) %>% 
    mutate(closest = as.integer(d_gh == min(d_gh))) %>% 
    ungroup()
```

```{r, eval = FALSE}
samp <- sample(df_choice$key,20000)
df_ <- df_choice %>% filter(key %in% samp) %>% 
    dummy_cols("teaching") %>% 
    select(choice,d_gh,teaching_nonteaching,teaching_major,zip,key,hosp_id,age,race,pay1, closest) %>% 
    na.omit()

fit <- clogit(choice ~  log(d_gh^2)  + age + factor(race) + factor(pay1) + closest + teaching_nonteaching + teaching_major + strata(zip) , data = df_, method=c("efron"), robust = TRUE)
df_$pr_select <- predict(fit,type = "expected")

df_ %>% 
    group_by(zip,hosp_id) %>% 
    summarise(n = sum(choice), p_n = sum(pr_select)) %>% 
    group_by(zip) %>% 
    mutate(N = sum(n), p_N = sum(p_n)) %>% 
    mutate(market_share = n/N) %>% 
    mutate(p_market_share = p_n / p_N) %>% 
    filter(market_share>0.01)

names(coef(fit))[-1] %>% map(~({
    c(coef(fit)[.x]/coef(fit)["log(d_gh^2)"],exp(coef(fit)[.x]))
}))

```


<!-- # Introduction -->

<!-- We begin by specifying a discrete choice model for patient i selecting -->
<!-- hospital h, though h can also represent the choice of a physician. To -->
<!-- ease expositional burden, we suppress indexing by time and disease -->
<!-- category, though in practice we will fit separate choice models for each -->
<!-- combination of Medicare population (TM, MA) and year. Using a standard -->
<!-- discrete choice framework, we specify the patient’s utility as -->

<!-- $$ -->
<!-- u_{i h}=\alpha_0+\alpha_1 X_i+\alpha_2 Q_h+\alpha_3 \log \left(t_{i h}\right)+v_{i h} -->
<!-- $$ {#eq-utility} -->

<!-- where $X_i$ capture patient-level attributes (e.g., demographics, -->
<!-- comorbidities, etc.), $Q_h$ capture hospital-level attributes (e.g., -->
<!-- measures of clinical quality), $t_{ih}=D_{ih}^2$, the squared -->
<!-- distance/travel time (D_ih) between i and h, and $v_{ih}$ is an -->
<!-- idiosyncratic error term. -->

<!-- It can be shown (and we have verified in preliminary analyses) that Eq. -->
<!-- 2 is equivalent to a gravity model in which the “forces” governing -->
<!-- patients’ selection of providers are informed by individual and facility -->
<!-- characteristics and the gravitational “pull” of geographically proximate -->
<!-- facilities.[@anas1983] -->

<!-- $$ -->
<!-- F_{i h}=G \frac{\widetilde{X_i}^{\alpha_1} \widetilde{Q_h}^{\alpha_2}}{t_{i h}^{-\alpha_3}} -->
<!-- $$ #{eq-gravity} where $\widetilde{X_i}=exp(X_i)$, $\widetilde{Q_i}=exp(Q_i)$ and $G$ is a constant.  -->


<!-- Under the information-maximizing approach, we want to define a model tha thas the most random predictions of indiviudal choices that are consistent with observeations in teh real world. So if 75% of patients are observed to go to A, the information-minimizing (entropy maximizing) choice probabilities shoudl be 0.75 [A] and 0.25 [B].  -->




