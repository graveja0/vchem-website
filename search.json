[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Main Website",
    "section": "",
    "text": "DOIs for blog posts"
  },
  {
    "objectID": "blog/posts/mvpf-evsi/mvpf-evsi.html",
    "href": "blog/posts/mvpf-evsi/mvpf-evsi.html",
    "title": "Welfare Analysis Meets Study Design",
    "section": "",
    "text": "Part three in a three-part series on how the tools of comparative welfare analysis can be used to refine and design prospective randomized evaluations.\nReference"
  },
  {
    "objectID": "blog/posts/mvpf-evsi/mvpf-evsi.html#what-is-the-expected-value-of-sample-information",
    "href": "blog/posts/mvpf-evsi/mvpf-evsi.html#what-is-the-expected-value-of-sample-information",
    "title": "Welfare Analysis Meets Study Design",
    "section": "What is the Expected Value of Sample Information?",
    "text": "What is the Expected Value of Sample Information?\nTo build an intuitive foundation for what follows, suppose we decide to design a randomized experiment that will improve our knowledge of MVPF parameter(s) \\(\\pi_z\\).33 Note that all other remaining MVPF parameters in \\(\\mathbf{\\pi}\\) are captured in \\(\\mathbf{\\pi_{-z}}\\), such that \\(\\mathbf{\\pi} = (\\mathbf{\\pi_{z}},\\mathbf{\\pi_{-z}})\\).\nWe traditionally think about the design of randomized experiments in terms of their statistical power—that is, the probability that a statistical test will detect a difference between treated and untreated groups, when such a difference exists.\nAnother way to state the above is that we want to avoid Type II error. But there are often practical (often resource-based) limits to how far we are willing to go on this.\nOne way to minimize Type II error is simply to enroll thousands or even millions of study units (e.g., people, households, clinics, etc.). But while that would yield a trial with power at or near 100%, our trial would be costly, time-consuming, and resource intensive—and we may well incur an additional loss in social welfare if an efficacious policy is (randomly) withheld from a large fraction of the overall population.\nConsequently, it is common to think of trial design in terms of enrolling enough study units so that a comparison of outcomes between treated and treated units has least 80% power to detect an effect. There is a vast statistical literature specifying analytic formulas and simulation-based exercises to optimize this (statistical) dimension of experimental design.\nA useful way to think about the EVSI is as the social welfare analog to power. That is, we want to avoid implementing a policy that results in a sub-optimal level of social welfare. It may be that doing nothing (or implementing some alternative policy option) would be the welfare-optimizing choice–but the available (uncertain) evidence leads us down the wrong decision path. To avoid this, we can leverage the EVSI to guide our study design so that we can optimize societal payoff of both the research itself, and the resulting policy decision.\nFrom a design perspective, the EVPPI for our parameter(s) of interest (\\(\\pi_z\\)) provides an upper bound on the information value that would accrue from obtaining perfect information on the “true” value(s) of \\(\\pi_z\\). In that sense, designing a trial that enrolls enough people or units to obtain an EVSI that is very close to the EVPPI is like designing a trial with statistical power at or near 100%.\nBy the same token, we can also think about designing a smaller-scale experiment that provides some new information value, but not (near) perfect information. But just as we want to avoid an under-powered trial, we also want to avoid a situation where the information value we obtain from our study is not enough to inform the policymaker’s decision.\nCalculating the EVSI for different trial sizes (and designs) allows us to do just this."
  },
  {
    "objectID": "blog/posts/mvpf-evsi/mvpf-evsi.html#expected-value-of-perfect-information",
    "href": "blog/posts/mvpf-evsi/mvpf-evsi.html#expected-value-of-perfect-information",
    "title": "Welfare Analysis Meets Study Design",
    "section": "Expected Value of Perfect Information",
    "text": "Expected Value of Perfect Information\nOur first guidepost is to make sure that any future research would help inform our policymaker’s decision.6 To assess this, we estimate the EVPI for various societal willingness-to-pay parameter (\\(\\lambda\\)) values. Figure 1 summarizes this exercise.6 We covered the EVPI in detail in part one of this series, however both the formulas and code are provided here for the example at hand.\n\n\n\n\n\n\nEVPI Equations and Notation (click to expand)\n\n\n\n\n\nPolicy benefits are summarized by \\(W(\\mathbf{\\pi},\\alpha)\\), and costs by \\(C(\\mathbf{\\pi},\\alpha)\\). The vector \\(\\mathbf{\\pi}\\) captures welfare-relevant parameters such as the average willingness-to-pay for the policy, program costs, and any fiscal externalities (FEs) that occur as a consequence of the policy.\nWe define and measure welfare benefits and costs for each of \\(D\\) total policy strategies \\(\\boldsymbol{\\alpha} = (\\alpha_1, \\cdots , \\alpha_D)\\).\nFinally, \\(\\lambda\\) summarizes societal willigness-to-pay (WTP), i.e., a policy with MVPF greater than or equal to \\(\\lambda\\) passes a societal cost-benefit test. This threshold value could simply be based on an MVPF of one (i.e., the benefit must be at least as great as the cost). Or, if society values some redistribution, \\(\\lambda\\) could be set based on a value less than one.\nThe net welfare benefit (NWB) is defined as:\n\\[\nNWB(\\boldsymbol{\\pi},\\alpha,\\lambda) = W(\\boldsymbol{\\pi},\\alpha) - \\lambda \\cdot C(\\boldsymbol{\\pi},\\alpha)\n\\tag{1}\\]\nand we define the optimal strategy as\n\\[\n\\alpha^* =\\arg\\max E_{\\boldsymbol{\\pi}} \\big [ NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) \\big ]\n\\]\nwhere \\(E_{\\pi}\\) is the expectation over the joint uncertainty distribution of current knowledge. Thus, \\(NWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda)\\) captures the net welfare benefit evaluated at the optimal strategy.\nThe Expected Value of Perfect Information (EVPI) is given by:\n\\[\nEVPI(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) = E_{\\boldsymbol{\\pi}} \\big [ \\max_{\\boldsymbol{\\alpha}} NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) \\big ] - NWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda)\n\\tag{2}\\]\n\n\n\n\n\nCode\ncalc_nwb &lt;- function(B,C,lambda) {\n    B - lambda * C\n}\n\ncalc_evpi &lt;- function(B,      # Benefits\n                      C,      # Net Costs\n                      lambda  # Societal willingness-to-pay \n                     ) {\n    nwb_ &lt;- calc_nwb(B = B, C = C, lambda = lambda)\n    evpi &lt;- mean(pmax(0,nwb_))-max(0,mean(nwb_)) \n    return(tibble(lambda = lambda, evpi = evpi))\n}\n\nevpi_data &lt;- \n    lambdas %&gt;% \n    map_df(~(calc_evpi(B = delta_W, C = (delta_E - delta_C), lambda = .x)))\n\nxlab &lt;- expression(paste(\"MVPF Value of Societal Willingness-to-Pay (\",lambda,\")\"))\n  \np &lt;- \n    evpi_data %&gt;% \n    ggplot(aes(x = lambda, y = evpi)) + \n    geom_point(size = 4) + geom_line(linewidth=1.25) + \n    theme_hc() + \n    theme(text = element_text(family = 'Arial')) +\n    labs(x = xlab, y = \"Value of Information\\nEVPI\")\n    #labs(x = \"Societal WTP Value (lambda)\", y = \"Value of Information (EVPI, EVPPI)\")\np\n\n\n\n\n\nFigure 1: Expected Value of Perfect Information by Societal Willingness to Pay Value\n\n\n\n\nFigure 1 shows that there is overall value in reducing information uncertainty in the policy decision—and this information value is maximized near a \\(\\lambda\\) value of 0.85.77 To provide some context for this value, Hendren (2016) estimates that the the MVPF for the Earned Income Tax Credit (EITC)—a popular means-tested cash transfer program—is around 0.9. It is worth noting, however, that later updates center the EITC MVPF estimate at 1.12."
  },
  {
    "objectID": "blog/posts/mvpf-evsi/mvpf-evsi.html#expected-value-of-partial-perfect-information",
    "href": "blog/posts/mvpf-evsi/mvpf-evsi.html#expected-value-of-partial-perfect-information",
    "title": "Welfare Analysis Meets Study Design",
    "section": "Expected Value of Partial Perfect Information",
    "text": "Expected Value of Partial Perfect Information\nOur next task is to prioritize future research around efforts to obtain information on parameter(s) with high information value. To do this, we will take probabilistic draws from the joint uncertainty distribution of the MVPF paramters in Table 1 and fit flexible metamodels for each parameter, with the calculated net welfare benefit as the outcome.88 Details on how to do this are provided in part two.\n\n\n\n\n\n\nEVPPI Equations and Notation\n\n\n\n\n\nSuppose we could obtain perfect information on some subset (\\(\\boldsymbol{\\pi}_z\\)) of the parameter(s) that determine the MVPF; we do not pursue additional research on the remaining uncertain parameters \\(\\boldsymbol{\\pi}_{-z}\\).\nThe expected value of partial perfect information (EVPPI) is defined as:\n\\[\nEVPPI(\\boldsymbol{\\pi}_z,\\lambda) = E_{\\boldsymbol{\\pi_z}} \\big [  \\max_{\\boldsymbol{\\alpha}} E_{\\boldsymbol{\\pi_{-z}}|\\boldsymbol{\\pi_z}}NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi}_z,\\boldsymbol{\\pi}_{-z},\\lambda) \\big ] -\n\\max E_{\\boldsymbol{\\pi}} \\big [ NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) \\big ]\n\\tag{3}\\]\n\n\n\nBased on this exercise, we can calculate and plot the EVPPI value for each MVPF parameter for various \\(\\lambda\\) values:\n\n\nCode\nest_evppi &lt;- function(B ,C,pi,lambda) {\n    nwb_ &lt;- calc_nwb(B = B, C = C, lambda = lambda)\n    \n    evppi_2 &lt;- pmax(0,mean(nwb_))\n    \n    evppi &lt;- list()\n    \n    names(pi) %&gt;% \n        map(~{\n            z &lt;- pi[[.x]]\n            mm.fit &lt;- gam(nwb_ ~ te(z))\n            evppi_inner &lt;- pmax(0, mm.fit$fitted)\n            evppi_1 &lt;- mean(evppi_inner)\n            evppi[[.x]] &lt;- evppi_1 - evppi_2\n        }) %&gt;% \n        set_names(names(pi)) %&gt;% \n        unlist()\n\n}\n\nevppi_lambdas &lt;- \n    lambdas %&gt;% \n    map(~(est_evppi(B = delta_W, C = (delta_E - delta_C), pi = pi, lambda = .x))) %&gt;% \n    bind_rows() %&gt;% \n    mutate(lambda = lambdas)\ncolnames(evppi_lambdas) &lt;- gsub(\"delta_\",\"\",colnames(evppi_lambdas))\n\np2 &lt;- \n    evppi_lambdas %&gt;% \n    gather(z,value,-lambda) %&gt;% \n    mutate(z = gsub(\"delta_\",\"\",z)) %&gt;% \n    ggplot() + geom_line(aes(x = lambda, y = value, colour = z )) + \n    ggsci::scale_color_d3() + \n    theme_hc() + \n    theme(text = element_text(family = 'Arial')) +\n    labs(x = xlab, y = \"Value of Information (EVPI, EVPPI)\")\n\ndirect.label(p2,list(method = \"top.bumponce\" , fontface=\"bold\", fontfamily ='Arial'))\n\n\n\n\n\nFigure 2: Expected Value of Partial Perfect Information by Societal Willingness to Pay Value\n\n\n\n\nFigure 2 shows that for \\(\\lambda\\) values near 0.85, the parameter summarizing the welfare benefit of the policy has the highest (partial) information value—though the taxpayer benefit parameter (\\(C\\)) also has high information value, especially for higher \\(\\lambda\\) values."
  },
  {
    "objectID": "blog/posts/mvpf-evsi/mvpf-evsi.html#what-happens-with-different-lambda-values",
    "href": "blog/posts/mvpf-evsi/mvpf-evsi.html#what-happens-with-different-lambda-values",
    "title": "Welfare Analysis Meets Study Design",
    "section": "What Happens with Different \\(\\lambda\\) values?",
    "text": "What Happens with Different \\(\\lambda\\) values?\nThe exercise above was carried out with a single societal willingness-to-pay parameter value in mind (\\(\\lambda=0.85\\)) but it is straightforward to carry out same exercise for different \\(\\lambda\\) values.\n\n\nCode\np_evsi2 &lt;- \n    evsi %&gt;%\n    bind_rows(evsi2) %&gt;% \n  mutate(evppi = ifelse(lambda ==0.85, evppi_w, evppi_w2)) %&gt;% \n  mutate(lab = glue::glue(\".  {n} ({round(100*value/evppi,1)}%)\")) %&gt;% \n    ggplot(aes(x = n, y = value, colour = factor(lambda))) + geom_point() + geom_line() + theme_hc() + \n    ggsci::scale_color_d3()+\n    geom_hline(aes(yintercept = evppi_w), colour = \"darkred\") + geom_text(aes(label = lab,hjust=0)) + \n    geom_hline(aes(yintercept = evppi_w2), colour = \"darkred\") +\n    theme(legend.position = \"none\") +\n    labs(x = \"Experimental Sample Size\", y = \"Expected value of Sample Information (EVSI)\") + \n    annotate(\"text\",x = 0, y = evppi_w, label = glue::glue(\"EVPPI (lambda = 0.85): {round(evppi_w,2)}\"),vjust=-1,hjust=0, colour= \"darkred\") +\n   annotate(\"text\",x = 0, y = evppi_w2, label = glue::glue(\"EVPPI (lambda = 1.0): {round(evppi_w2,2)}\"),vjust=-1,hjust=-1, colour= \"darkred\") +\n    scale_y_continuous(expand = c(.25,0)) + theme(text = element_text(family = 'Arial')) +\n  scale_x_continuous(expand=expansion(mult = c(0, .35)),breaks = c(100,250, 500, 1000,2000, 5000, 10000),guide = guide_axis(n.dodge=3))\np_evsi2\n\n\n\n\n\nFigure 4: Expected Value of Sample Information (% of EVPPI) by Societal Willingness to Pay Value"
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "",
    "text": "Part one in a three-part series on how the tools of comparative welfare analysis can be used to refine and design prospective randomized evaluations."
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#introduction",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#introduction",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "Introduction",
    "text": "Introduction\nSuppose a policymaker is considering strategies to reduce poverty. For the sake of simplicity, let’s assume the policymaker is weighing the tradeoffs of creating or expanding an in-kind benefit transfer, such as a job training program.\nImplementing the program has a measurable impact on welfare-relevant outcomes (e.g., education, lifetime earnings, health, etc.). However, the program also imposes costs on society. These costs reflect both the budgetary cost (i.e., the total sum of program expenditures), as well as other changes in behavior, expenditures, or tax revenues that either bolster or drag on social welfare.\nBefore we proceed, it is useful to formalize the above and define summary measures of policy benefits \\(W(\\mathbf{\\pi},\\alpha)\\) and costs \\(C(\\mathbf{\\pi},\\alpha)\\). The vector \\(\\mathbf{\\pi}\\) captures welfare-relevant parameters such as the average willingness-to-pay for the policy, program costs, and any fiscal externalities (FEs) that occur as a consequence of the policy.1 We define and measure welfare benefits and costs for each of \\(D\\) total policy strategies \\(\\boldsymbol{\\alpha} = (\\alpha_1, \\cdots , \\alpha_D)\\).1 For example, these fiscal externalities might include changes to government tax revenues stemming from changes in participation in the labor force and/or other social programs.\nIn a series of influential papers, Hendren and colleagues define the Marginal Value of Public Funds (MVPF) as the ratio of benefits to costs (Finkelstein and Hendren 2020; Hendren and Sprung-Keyser 2022, 2022; Hendren 2016):\n\\[\nMVPF(\\mathbf{\\pi},\\alpha) = \\frac{W(\\mathbf{\\pi},\\alpha)}{C(\\mathbf{\\pi},\\alpha)}\n\\tag{1}\\]\nThe MVPF measures the marginal value of an additional dollar spent on a policy. That is, the MVPF quantifies how the welfare benefits accrued by implementing a policy compare to the costs of adopting it.\nWhile we will leave the theoretical details to the aforementioned citations, we can summarize the MVPF of an in-kind benefit transfer as:\n\\[\nMVPF^{\\text{inkind}}(\\mathbf{\\pi},\\alpha) = \\frac{W(\\mathbf{\\pi},\\alpha)}{1+FE(\\mathbf{\\pi},\\alpha)}\n\\tag{2}\\] where \\(FE(\\mathbf{\\pi},\\alpha)\\) is the fiscal externality associated with the policy, and \\(W(\\mathbf{\\pi},\\alpha)\\) is the willingness to pay among infra-marginal recipients of the program.\nIn this example, we are comparing a single policy strategy (in-kind benefit program) to an alternative strategy of doing nothing. However, in principle we could think of alternative (competing) strategies to achieve the same objective—each with its own welfare cost and benefit estimates—that might be under consideration."
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#the-state-of-current-knowledge",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#the-state-of-current-knowledge",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "The State of Current Knowledge",
    "text": "The State of Current Knowledge\nSuppose that previous research has quantified the welfare benefits and costs—such that the hypothetical policymaker has at her disposal some information that can guide her decision on which policy (if any) to pursue.\nThe state of current information is summarized in Table 1.\n\n\n\n\nTable 1: Summary of Current Knowledge\n\n\n\nMVPF [95% interval]\nBenefits [95% Interval]\nCosts [95% Interval]\n\n\n\n\ninkind\n0.89 [0.73,1.08]\n1.21 [0.99,1.46]\n1.35 [0.31,0.39]"
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#mvpf-as-a-decision-tool",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#mvpf-as-a-decision-tool",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "MVPF as a Decision Tool",
    "text": "MVPF as a Decision Tool\nTable 1 highlights that the policymaker’s decision is fraught with uncertainty. Based on current information, the estimated MVPF of the in-kind benefit is 0.894. However, the 95% interval shows that current knowledge is consistent with the policy more than paying for itself (i.e., an MVPF above 1), and with the policy resulting in some redistribution (i.e., an MVPF below 1).\nIn short, based on current evidence, it is hard to say for sure whether the policy should be adopted.\nOur policymaker’s decision problem actually goes one step further. The MVPF summarizes the private benefits that accrue from the in-kind benefit transfer; it may be that social preferences are such that, from a societal perspective, a policy with an MVPF below 1.0 is not worth pursuing.\nWe can formalize this idea by defining an additional parameter \\(\\lambda\\) summarizing societal willigness-to-pay (WTP). This threshold value could simply be based on an MVPF of one (i.e., the benefit must be at least as great as the cost). Or, if society values some redistributive consequence of the policy, \\(\\lambda\\) could be set based on a value less than one.22 For example, Finkelstein, Hendren, and Shepard (2019) make comparative assessments of health insurance subsidization policies by specifying a social welfare function over Constant Relative Risk Aversion (CRRA) utility and a defined coefficient of risk aversion (\\(\\sigma = 3\\)). This results in \\(\\lambda = 0.2\\). But researchers do not necessarily have to specify the structure of the social welfare function to define a decision-making benchmark. A value tied to an existing policy with strong social support could also suffice. For instance, Finkelstein, Hendren, and Shepard (2017) also consider a benchmark (\\(\\lambda = 0.88\\)) based on the MVPF of the Earned Income Tax Credit (EITC)—a popular means-tested cash transfer program. Finally, Hendren (2020) argues for the use of efficient welfare weights that project the welfare costs and benefits of any specific policy into the MVPF of a tax transfer between the affected populations.\nFigure 1 visualizes this uncertainty for 1,000 draws from the uncertainty distributions of \\(W^{\\text{inkind}}(\\mathbf{\\pi},\\alpha)\\) and \\(C^{\\text{inkind}}(\\mathbf{\\pi},\\alpha)\\) in Table 1.3 In addition, for three hypothetical values of \\(\\lambda\\), each point is shaded based on whether the implied MVPF is above or below \\(\\lambda\\).3 For the sake of this example, welfare benefits are drawn from a lognormal distribution with mean \\(\\log(1.2)\\) and standard deviation \\(0.1\\). Welfare costs are based on Equation 2 and a fiscal externality that is normally distributed with mean \\(0.35\\) and standard deviation \\(0.2\\).\nThe dotted lines show a ray from the origin with slope \\(1/\\lambda\\); values that fall under and to the right of the line are those where the MVPF\\(\\geq \\lambda\\) and those that fall up and to the left of the line are those where MVPF\\(&lt;\\lambda\\). Finally, the green diamonds plot the average welfare benefit and cost values as summarized in Table 1.\n\n\n\n\n\n\n\n(a) lambda = 1.0\n\n\n\n\n\n\n\n(b) lambda = 0.90\n\n\n\n\n\n\n\n\n\n(c) lambda = 0.5\n\n\n\n\nFigure 1: Welfare Cost and Benefits Overlaid With Decision Rules Based on \\(\\lambda\\)\n\n\nFigure 1 (a) highlights that when \\(\\lambda=1\\), the MVPF of the in-kind benefit strategy rarely exceeds the societal willingness-to-pay threshold. In other words, there are few points in the joint uncertainty distribution where the value falls below and to the right of the dotted line. Thus, there is some—but not much—uncertainty in the decision to not implement the policy under this decision criterion.\nBy comparison, Figure 1 (c) shows that when \\(\\lambda=0.5\\), there is no decision uncertainty; our policymaker would conclude that the the in-kind benefit should be adopted—and this decision is consistent across all values in the uncertainty ranges."
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#the-opportunity-cost-of-imperfect-information",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#the-opportunity-cost-of-imperfect-information",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "The Opportunity Cost of Imperfect Information",
    "text": "The Opportunity Cost of Imperfect Information\nThe above discussion highlights that depending on social preferences, policy decisions based on current information may carry an opportunity cost of making the wrong decision. This is most evident in Figure 1 (b), where based on the average welfare cost estimates in Table 1 (plotted as a green dot in Figure 1 (b)), the policymaker might elect to pursue an in-kind benefit program. In other words, the implied average MPVF basd on these values is 0.894. If \\(\\lambda = 0.9\\), this value is below the WTP threshold—so the policy would appear to pass a societal cost-benefit test.\nHowever, Figure 1 (b) also makes clear the available information is also consistent with the “true” MVPF falling above \\(\\lambda\\). That is, roughly half of the points are shaded gold (i.e., the policy does not pass our defined societal cost-benefit test) and half are shaded black (i.e., MVPF\\(&lt;\\lambda\\)). If the true MVPF is below \\(\\lambda\\), then society would incur a net welfare loss from implementing the policy.\nContrast this with the scenario in Figure 1 (c), which is based on \\(\\lambda = 0.5\\). In that scenario, there is uncertainty in the underlying welfare estimates but no uncertainty in policy adoption decisions. The same decision (to implement the in-kind benefit program) occurs across the entire uncertainty range, so there is no opportunity cost to making the “wrong” decision."
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#net-welfare-benefit",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#net-welfare-benefit",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "Net Welfare Benefit",
    "text": "Net Welfare Benefit\nLet’s now formalize the above observations by defining the net welfare benefit as the benefits of a given policy minus \\(\\lambda\\) multiplied by the costs:\n\\[\nNWB(\\boldsymbol{\\pi},\\alpha,\\lambda) = W(\\boldsymbol{\\pi},\\alpha) - \\lambda \\cdot C(\\boldsymbol{\\pi},\\alpha)\n\\tag{3}\\]\nIntuitively, policies where \\(NWB \\geq 0\\) indicate situations where the MVPF is equal to or greater than \\(\\lambda\\). Put simply, when the NWB is positive, the policy passes the societal cost-benefit test; when it is negative, it does not.\nWith the NWB defined, it is useful to think about uncertainty along two distinct but related dimensions:\n\nVariation in the NWB that derives from sampling/structural/modeling uncertainty in estimates of \\(W^{\\text{inkind}}(\\mathbf{\\pi},\\alpha)\\) and \\(C^{\\text{inkind}}(\\mathbf{\\pi},\\alpha)\\).\nVariation in optimal decisions, which occurs when variation from (1) causes frequent sign changes in the NWB.\n\nIn other words, depending on the value of \\(\\lambda\\), variation in welfare outcomes (costs and benefits) does not necessarily imply variation in optimal decisions; it may be \\(\\lambda\\) is sufficiently low (or high) that policy decisions do not vary across the uncertainty distribution.44 This is the scenario depicted in Figure 1 (c)."
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#from-net-welfare-benefit-to-value-of-information",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#from-net-welfare-benefit-to-value-of-information",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "From Net Welfare Benefit to Value of Information",
    "text": "From Net Welfare Benefit to Value of Information\nThe idea that decisions based on current information may carry a welfare-valued opportunity cost underlies decision theoretic concepts of the value of information (VOI).\nBefore we define various VOI concepts and measures, let’s lay out the practical questions a VOI-based approach to welfare analysis based on the MVPF can answer:\n\nWhich strategies are cost-effective given social preferences and based on current evidence? To answer this question, we can assess whether current knowledge–cast against a summary measure of social preferences such as \\(\\lambda\\)—is sufficient to confidently make policy decisions. Because we may not want to make a stance on the particular value of \\(\\lambda\\), we can vary these assessments over a plausible range of values and estimate the expected welfare loss of making the wrong decision at a given value of \\(\\lambda\\).\nShould we invest more resources to reduce uncertainty in our decisions? If so, it will be important to isolate which sources of uncertainty we should focus on. Furthermore, we want to think about the most efficient way to sample the population to improve our knowledge on these dimensions."
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#the-welfare-loss-from-imperfect-information",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#the-welfare-loss-from-imperfect-information",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "The Welfare Loss from Imperfect Information",
    "text": "The Welfare Loss from Imperfect Information\nTo answer the first question, we need a summary measure of the expected welfare loss from making the wrong decision.\nDefine the optimal strategy as\n\\[\n\\alpha^* =\\arg\\max E_{\\boldsymbol{\\pi}} \\big [ NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) \\big ]\n\\] so that \\(NWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda)\\) is the net welfare benefit evaluated at the optimal strategy, i.e.,\n\\[\nNWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda) = \\max_{\\boldsymbol{\\alpha}} E_{\\boldsymbol{\\pi}} \\big [  NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) \\big ]\n\\]\nNext, define a welfare loss function\n\\[\nL_{\\alpha} = \\max_{\\boldsymbol{\\alpha}} NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) - NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda)\n\\tag{4}\\]\nThe loss function evaluated at \\(\\alpha^*\\) is\n\\[\nL_{\\alpha^*} = \\max_{\\boldsymbol{\\alpha}} NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) - NWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda)\n\\tag{5}\\]\nAnd its expected value is given by\n\\[\nE_{\\boldsymbol{\\pi}} \\big [ L_{\\alpha^*} \\big ]  = E_{\\boldsymbol{\\pi}} \\big [ \\max_{\\boldsymbol{\\alpha}} NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda) \\big ] - NWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda)\n\\tag{6}\\]\nThe expected welfare loss in Equation 6 is also known as the expected value of perfect information (Jackson et al. 2022; Wilson 2015):\n\\[\nE_{\\boldsymbol{\\pi}} \\big [ L_{\\alpha^*} \\big ]  = EVPI(\\boldsymbol{\\alpha},\\boldsymbol{\\pi},\\lambda)\n\\] ### Estimating the Expected Welfare Loss\nThe code below calcualtes the EVPI for the MVPF of the in-kind benefit transfer above, for a particular value of \\(\\lambda\\):\n\nset.seed(23)\nn &lt;- 1e5   # Number of uncertainty distribution draws\nw &lt;- rlnorm(n, log(1.2),0.1)   # welfare benefit is lognormal(ln(1.2),0.1)\nfe &lt;- rnorm(n, 0.35,.02)  # fiscal externality is norm(0.35,0.02)\nc &lt;- 1+fe  # cost is denominator of MVPF\n\nlambda &lt;- 0.9  # set a societal WTP value\nnwb &lt;- w - lambda * c  # calculate the net welfare benefit\nevpi &lt;-mean(pmax(0,nwb))-max(0,mean(nwb))  # expected value of perfect information\nevpi\n\n[1] 0.04441101\n\n\nThe estimated EVPI (0.044) is non-zero, indicating that there is overall value in reducing uncertainty in the policymaker’s decision to implement the in-kind benefit transfer. However, note that the EVPI varies over different values of \\(\\lambda\\). If \\(\\lambda\\) were instead 0.5, we would make the same decision (to implement the policy) across the entire uncertainty range—so there is no value in obtaining new information on uncertain inputs into the MVPF estimate. This can be seen in the code below:\n\nlambda &lt;- 0.5 # set a societal WTP value\nnwb_alt &lt;- w - lambda * c  # calculate the net welfare benefit\nevpi_alt &lt;-mean(pmax(0,nwb_alt))-max(0,mean(nwb_alt))  # expected value of perfect information\nevpi_alt\n\n[1] 0\n\n\nFigure 2 is based on repeating the above exercise across a variety of \\(\\lambda\\) values (x-axis) and plotting the resulting EVPI estimate for each (y-axis):\n\n\n\n\n\n\nFigure 2: Expected Value of Reducing Decision Uncertainty Through Future Research, by Societal Willingness-to-Pay Value (lambda)"
  },
  {
    "objectID": "blog/posts/mvpf-evpi/mvpf-evpi.html#summary-and-next-steps",
    "href": "blog/posts/mvpf-evpi/mvpf-evpi.html#summary-and-next-steps",
    "title": "Welfare Analysis Meets Decision Theory",
    "section": "Summary and Next Steps",
    "text": "Summary and Next Steps\nFigure 2 tells us that based on existing evidence, we have an answer to our first question: if the societal willigness-to-pay value (\\(\\lambda\\)) is in the neighborhood of 0.75-1.15, then there is high information value in future research to reduce uncertainty in \\(W(\\mathbf{\\pi},\\alpha)\\) and \\(C(\\mathbf{\\pi},\\alpha)\\).\nWith this information in mind—and so long as pursuing more information has value—we can move on to the second question and start to ask (a) which specific sources of uncertainty (i.e., individual parameters or sets of parmeters in \\(W(\\mathbf{\\pi},\\alpha)\\) and \\(C(\\mathbf{\\pi},\\alpha)\\)) drive the overall EVPI estimate? And how can we prospectively design a clinical trial or randomized evaluation in such a way as to most efficiently reduce information uncertainty to a point where a better decision can be made? Answering these questions is the focus of part two in this series."
  },
  {
    "objectID": "blog/posts/mvpf-evppi/mvpf-evppi.html",
    "href": "blog/posts/mvpf-evppi/mvpf-evppi.html",
    "title": "Welfare Analysis Meets Research Prioritization",
    "section": "",
    "text": "Part two in a three-part series on how the tools of comparative welfare analysis can be used to refine and design prospective randomized evaluations."
  },
  {
    "objectID": "blog/posts/mvpf-evppi/mvpf-evppi.html#estimating-the-expected-value-of-partial-perfect-information",
    "href": "blog/posts/mvpf-evppi/mvpf-evppi.html#estimating-the-expected-value-of-partial-perfect-information",
    "title": "Welfare Analysis Meets Research Prioritization",
    "section": "Estimating the Expected Value of Partial Perfect Information",
    "text": "Estimating the Expected Value of Partial Perfect Information\nWe can estimate the EVPPI by drawing values from the joint uncertainty distributions of \\(W^{\\text{inkind}}(\\mathbf{\\pi},\\alpha)\\) and \\(C^{\\text{inkind}}(\\mathbf{\\pi},\\alpha)\\).\nSpecifically, suppose we construct a \\(K\\)-sized sample of model outputs from a Monte Carlo-based exercise. That is, we draw sets of parameters \\(\\boldsymbol{\\pi}^{(1)},\\ldots,\\boldsymbol{\\pi}^{(K)}\\) and generate a \\(K\\)-sized vector of net welfare benefits for a given \\(\\lambda\\) value.\nThe code below samples the constituent pieces for the MVPF and NWB for \\(\\lambda=0.88\\):\n\nset.seed(23)\nK &lt;- 1e5\nlambda = 0.88\nw_inkind = rlnorm(K, log(1.2),0.1)\nfe_inkind = rnorm(K, 0.35,.02)\nc_inkind = 1+fe_inkind\nnwb_inkind = w_inkind - lambda * c_inkind\n\n\n\n\n\nAt \\(\\lambda = 0.88\\) the in-kind benefit passes the societal cost-benefit test, so implementation of the in-kind benefit (as compared with doing nothing) is the optimal strategy based on current information. Therefore, the second term in Equation 3 (i.e., \\(E_{\\boldsymbol{\\pi}}[NWB(\\alpha^*,\\boldsymbol{\\pi},\\lambda)]\\)) is simply the average NWB of the in-kind benefit:\n\nevppi_2 &lt;- pmax(0,mean(nwb_inkind))\nevppi_2\n\n[1] 0.01831933\n\n\nEstimation of the conditional expectation in the first term in equation Equation 3 is less straightforward. Borrowing from the approach in Strong, Oakley, and Brennan (2014), we can express the NWB outcome as the sum of a conditional expectation plus a mean-zero error term:\n\\[\nNWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi^{(k)}},\\lambda)  =  E_{\\boldsymbol{\\pi_{-z}}|\\boldsymbol{\\pi_z}=\\boldsymbol{\\pi_z}^{(k)}}NWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi}_z^{(k)},\\boldsymbol{\\pi}_{-z},\\lambda)  + \\epsilon^{(k)}\n\\tag{4}\\]\nIn addition, the expectation in equation Equation 4 can be thought of in terms of an unknown function \\(g(\\cdot)\\) of \\(\\boldsymbol{\\pi_z}\\):\n\\[\nNWB(\\boldsymbol{\\alpha},\\boldsymbol{\\pi^{(k)}},\\lambda)  = g(\\boldsymbol{\\alpha},\\boldsymbol{\\pi}_z^{(k)},\\lambda) + \\epsilon^{(k)}\n\\tag{5}\\]\nBased on equation Equation 5, one option is to estimate the conditional expectation using a “metamodel,” or a regression model predicting how the net welfare benefit for a particular policy varies with unknown parameters of interest \\(\\boldsymbol{\\pi}_z\\). For example, a metamodel might specify the function \\(g(\\cdot)\\) as a standard linear regression. Alternatively, we might not wish to impose a functional form and instead estimate \\(g(\\cdot)\\) nonparametrically.33 If we suspect the underlying relationships have important nonlinearities in the parameters of interest, we could also appeal to machine learning methods to estimate \\(g(\\cdot)\\).\nThe code below estimates basic metamodels separately for welfare benefits and costs using a generalized additive model:44 In our running example, we have a scalar value for welfare benefits and costs, so the metamodels end up being quite simple in structure. In principle, however, multiple parameters might inform the overall welfare cost or benefit values—in which case we could think of estimating metamodels separately for each, or combine them all into a single metamodel with interaction terms, etc. to capture dependencies or nonlinearities among them in determining the overall welfare benefit or cost value.\n\n# Metamodel for welfare benefits\ngam.w &lt;- gam(nwb_inkind ~ w_inkind)\n\n# Metamodel for welfare costs\ngam.c &lt;- gam(nwb_inkind ~ c_inkind)\n\nBecause we are comparing the in-kind benefit policy to doing nothing (i.e., a policy with zero-valued welfare costs and benefits), the inner (square bracketed) term of the first term in Equation 3 can be estimated by taking the maximum of 0 and the predicted values from the metamodels:\n\nevppi_w_inner &lt;-pmax(0,gam.w$fitted)\nevppi_c_inner &lt;-pmax(0,gam.c$fitted)\n\nThe outer expectation of the first term in Equation 3 is estimated using the average across the \\(K\\) probabilistic draws:\n\nevppi_w_1 = mean(evppi_w_inner)\nevppi_c_1 = mean(evppi_c_inner)\n\nAnd finally, the EVPPI for welfare costs and benefits is estimated by the difference:\n\nevppi_w = evppi_w_1 - evppi_2\nevppi_c = evppi_c_1 - evppi_2\n\nc(\"EVPPI_W\" = evppi_w, \n  \"EVPPI_C\" = evppi_c)\n\n    EVPPI_W     EVPPI_C \n0.038941735 0.001417349 \n\n\nFrom these estimates of the EVPPI we would conclude that the component of \\(\\mathbf{\\pi}\\) with the highest information value is \\(W(\\boldsymbol{\\pi},\\alpha)\\)."
  },
  {
    "objectID": "blog/posts/mvpf-evppi/mvpf-evppi.html#next-steps",
    "href": "blog/posts/mvpf-evppi/mvpf-evppi.html#next-steps",
    "title": "Welfare Analysis Meets Research Prioritization",
    "section": "Next Steps",
    "text": "Next Steps\nA this point we have:\n\nExplored the range of current knowledge to ascertain whether future research could help a policymaker’s decision to pursue an in-kind benfefit transfer program.\nDecomposed the value of information to identify the parameters with highest decision leverage.\n\nAs mentioned above, however, our estimates of the value of information are based on a theoretical exercise under which we obtain perfect information on uncertain parameters. In practice, however, we do not have unlimited resources to reduce information uncertainty to zero; we must weigh the costs and benefits of sampling the population—perhaps by carrying out a randomized evaluation of the in-kind benefit transfer—to obtain better, but not perfect, information.\nThe next post in this series will explore different study designs so that we can optimally design a randomized evaluation in such a way as to efficiently gain information to inform the decision."
  },
  {
    "objectID": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html",
    "href": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html",
    "title": "Quantile Treatment Effects",
    "section": "",
    "text": "The objective of this workbook is to work through estimation of quantile treatment effects using the Lalonde experimental and observational data."
  },
  {
    "objectID": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#description-of-data",
    "href": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#description-of-data",
    "title": "Quantile Treatment Effects",
    "section": "Description of Data",
    "text": "Description of Data\nOur example data will be drawn from the infamous Lalonde study:\n\nRobert Lalonde, “Evaluating the Econometric Evaluations of Training Programs”, American Economic Review, Vol. 76, pp. 604-620\n\nFrom Dehejia and Wahba (1999):\n\nLalonde estimated the impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention income levels. He used data from a randomized evaluation of the program and examined the extent to which nonexperimental estimators can replicate the unbiased experimental estimate of the treatment impact when applied to a composite dataset of experimental treatment units and nonexperimental comparison units."
  },
  {
    "objectID": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#experimental-data",
    "href": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#experimental-data",
    "title": "Quantile Treatment Effects",
    "section": "Experimental Data",
    "text": "Experimental Data\n\nlalonde.exp %&gt;% \n  group_by(treat) %&gt;% \n  skim()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nPiped data\n\n\n\n\nNumber of rows\n\n\n445\n\n\n\n\nNumber of columns\n\n\n13\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n12\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\ntreat\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\ntreat\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nage\n\n\n0\n\n\n0\n\n\n1\n\n\n25.05\n\n\n7.06\n\n\n17\n\n\n19.00\n\n\n24.0\n\n\n28.00\n\n\n55\n\n\n▇▅▂▁▁\n\n\n\n\nage\n\n\n1\n\n\n0\n\n\n1\n\n\n25.82\n\n\n7.16\n\n\n17\n\n\n20.00\n\n\n25.0\n\n\n29.00\n\n\n48\n\n\n▇▇▂▁▁\n\n\n\n\neducation\n\n\n0\n\n\n0\n\n\n1\n\n\n10.09\n\n\n1.61\n\n\n3\n\n\n9.00\n\n\n10.0\n\n\n11.00\n\n\n14\n\n\n▁▁▃▇▂\n\n\n\n\neducation\n\n\n1\n\n\n0\n\n\n1\n\n\n10.35\n\n\n2.01\n\n\n4\n\n\n9.00\n\n\n11.0\n\n\n12.00\n\n\n16\n\n\n▁▂▇▃▁\n\n\n\n\nblack\n\n\n0\n\n\n0\n\n\n1\n\n\n0.83\n\n\n0.38\n\n\n0\n\n\n1.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▂▁▁▁▇\n\n\n\n\nblack\n\n\n1\n\n\n0\n\n\n1\n\n\n0.84\n\n\n0.36\n\n\n0\n\n\n1.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▂▁▁▁▇\n\n\n\n\nhispanic\n\n\n0\n\n\n0\n\n\n1\n\n\n0.11\n\n\n0.31\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n0.00\n\n\n1\n\n\n▇▁▁▁▁\n\n\n\n\nhispanic\n\n\n1\n\n\n0\n\n\n1\n\n\n0.06\n\n\n0.24\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n0.00\n\n\n1\n\n\n▇▁▁▁▁\n\n\n\n\nmarried\n\n\n0\n\n\n0\n\n\n1\n\n\n0.15\n\n\n0.36\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n0.00\n\n\n1\n\n\n▇▁▁▁▂\n\n\n\n\nmarried\n\n\n1\n\n\n0\n\n\n1\n\n\n0.19\n\n\n0.39\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n0.00\n\n\n1\n\n\n▇▁▁▁▂\n\n\n\n\nnodegree\n\n\n0\n\n\n0\n\n\n1\n\n\n0.83\n\n\n0.37\n\n\n0\n\n\n1.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▂▁▁▁▇\n\n\n\n\nnodegree\n\n\n1\n\n\n0\n\n\n1\n\n\n0.71\n\n\n0.46\n\n\n0\n\n\n0.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▃▁▁▁▇\n\n\n\n\nre74\n\n\n0\n\n\n0\n\n\n1\n\n\n2107.03\n\n\n5687.91\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n139.42\n\n\n39571\n\n\n▇▁▁▁▁\n\n\n\n\nre74\n\n\n1\n\n\n0\n\n\n1\n\n\n2095.57\n\n\n4886.62\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n1291.47\n\n\n35040\n\n\n▇▁▁▁▁\n\n\n\n\nre75\n\n\n0\n\n\n0\n\n\n1\n\n\n1266.91\n\n\n3102.98\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n650.10\n\n\n23032\n\n\n▇▁▁▁▁\n\n\n\n\nre75\n\n\n1\n\n\n0\n\n\n1\n\n\n1532.06\n\n\n3219.25\n\n\n0\n\n\n0.00\n\n\n0.0\n\n\n1817.28\n\n\n25142\n\n\n▇▁▁▁▁\n\n\n\n\nre78\n\n\n0\n\n\n0\n\n\n1\n\n\n4554.80\n\n\n5483.84\n\n\n0\n\n\n0.00\n\n\n3138.8\n\n\n7288.42\n\n\n39484\n\n\n▇▂▁▁▁\n\n\n\n\nre78\n\n\n1\n\n\n0\n\n\n1\n\n\n6349.15\n\n\n7867.40\n\n\n0\n\n\n485.23\n\n\n4232.3\n\n\n9643.00\n\n\n60308\n\n\n▇▁▁▁▁\n\n\n\n\nu74\n\n\n0\n\n\n0\n\n\n1\n\n\n0.75\n\n\n0.43\n\n\n0\n\n\n0.75\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▂▁▁▁▇\n\n\n\n\nu74\n\n\n1\n\n\n0\n\n\n1\n\n\n0.71\n\n\n0.46\n\n\n0\n\n\n0.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▃▁▁▁▇\n\n\n\n\nu75\n\n\n0\n\n\n0\n\n\n1\n\n\n0.68\n\n\n0.47\n\n\n0\n\n\n0.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▃▁▁▁▇\n\n\n\n\nu75\n\n\n1\n\n\n0\n\n\n1\n\n\n0.60\n\n\n0.49\n\n\n0\n\n\n0.00\n\n\n1.0\n\n\n1.00\n\n\n1\n\n\n▅▁▁▁▇\n\n\n\n\nid\n\n\n0\n\n\n0\n\n\n1\n\n\n315.50\n\n\n75.20\n\n\n186\n\n\n250.75\n\n\n315.5\n\n\n380.25\n\n\n445\n\n\n▇▇▇▇▇\n\n\n\n\nid\n\n\n1\n\n\n0\n\n\n1\n\n\n93.00\n\n\n53.55\n\n\n1\n\n\n47.00\n\n\n93.0\n\n\n139.00\n\n\n185\n\n\n▇▇▇▇▇"
  },
  {
    "objectID": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#observational-panel-data-from-the-panel-survey-of-income-dynamics",
    "href": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#observational-panel-data-from-the-panel-survey-of-income-dynamics",
    "title": "Quantile Treatment Effects",
    "section": "Observational (Panel) Data from the Panel Survey of Income Dynamics",
    "text": "Observational (Panel) Data from the Panel Survey of Income Dynamics\n\nlalonde.psid.panel %&gt;% \n  group_by(treat) %&gt;% \n  skim()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nPiped data\n\n\n\n\nNumber of rows\n\n\n8025\n\n\n\n\nNumber of columns\n\n\n13\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n1\n\n\n\n\nnumeric\n\n\n11\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\ntreat\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\ntreat\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\nuniqueid\n\n\n0\n\n\n0\n\n\n1\n\n\n8\n\n\n10\n\n\n0\n\n\n7470\n\n\n0\n\n\n\n\nuniqueid\n\n\n1\n\n\n0\n\n\n1\n\n\n6\n\n\n8\n\n\n0\n\n\n555\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\ntreat\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nyear\n\n\n0\n\n\n0\n\n\n1\n\n\n1975.67\n\n\n1.70\n\n\n1974\n\n\n1974\n\n\n1975.0\n\n\n1978.0\n\n\n1978\n\n\n▇▇▁▁▇\n\n\n\n\nyear\n\n\n1\n\n\n0\n\n\n1\n\n\n1975.67\n\n\n1.70\n\n\n1974\n\n\n1974\n\n\n1975.0\n\n\n1978.0\n\n\n1978\n\n\n▇▇▁▁▇\n\n\n\n\nid\n\n\n0\n\n\n0\n\n\n1\n\n\n2035.05\n\n\n2876.75\n\n\n186\n\n\n808\n\n\n1430.5\n\n\n2053.0\n\n\n23100\n\n\n▇▁▁▁▁\n\n\n\n\nid\n\n\n1\n\n\n0\n\n\n1\n\n\n93.00\n\n\n53.45\n\n\n1\n\n\n47\n\n\n93.0\n\n\n139.0\n\n\n185\n\n\n▇▇▇▇▇\n\n\n\n\nre\n\n\n0\n\n\n0\n\n\n1\n\n\n20015.33\n\n\n14260.04\n\n\n0\n\n\n10742\n\n\n19210.5\n\n\n27337.9\n\n\n156653\n\n\n▇▂▁▁▁\n\n\n\n\nre\n\n\n1\n\n\n0\n\n\n1\n\n\n3325.59\n\n\n6046.71\n\n\n0\n\n\n0\n\n\n0.0\n\n\n4574.5\n\n\n60308\n\n\n▇▁▁▁▁\n\n\n\n\nage\n\n\n0\n\n\n0\n\n\n1\n\n\n34.85\n\n\n10.44\n\n\n18\n\n\n26\n\n\n33.0\n\n\n44.0\n\n\n55\n\n\n▇▇▆▅▅\n\n\n\n\nage\n\n\n1\n\n\n0\n\n\n1\n\n\n25.82\n\n\n7.14\n\n\n17\n\n\n20\n\n\n25.0\n\n\n29.0\n\n\n48\n\n\n▇▇▂▁▁\n\n\n\n\neducation\n\n\n0\n\n\n0\n\n\n1\n\n\n12.12\n\n\n3.08\n\n\n0\n\n\n11\n\n\n12.0\n\n\n14.0\n\n\n17\n\n\n▁▁▃▇▅\n\n\n\n\neducation\n\n\n1\n\n\n0\n\n\n1\n\n\n10.35\n\n\n2.01\n\n\n4\n\n\n9\n\n\n11.0\n\n\n12.0\n\n\n16\n\n\n▁▂▇▃▁\n\n\n\n\nblack\n\n\n0\n\n\n0\n\n\n1\n\n\n0.25\n\n\n0.43\n\n\n0\n\n\n0\n\n\n0.0\n\n\n1.0\n\n\n1\n\n\n▇▁▁▁▃\n\n\n\n\nblack\n\n\n1\n\n\n0\n\n\n1\n\n\n0.84\n\n\n0.36\n\n\n0\n\n\n1\n\n\n1.0\n\n\n1.0\n\n\n1\n\n\n▂▁▁▁▇\n\n\n\n\nhispanic\n\n\n0\n\n\n0\n\n\n1\n\n\n0.03\n\n\n0.18\n\n\n0\n\n\n0\n\n\n0.0\n\n\n0.0\n\n\n1\n\n\n▇▁▁▁▁\n\n\n\n\nhispanic\n\n\n1\n\n\n0\n\n\n1\n\n\n0.06\n\n\n0.24\n\n\n0\n\n\n0\n\n\n0.0\n\n\n0.0\n\n\n1\n\n\n▇▁▁▁▁\n\n\n\n\nmarried\n\n\n0\n\n\n0\n\n\n1\n\n\n0.87\n\n\n0.34\n\n\n0\n\n\n1\n\n\n1.0\n\n\n1.0\n\n\n1\n\n\n▁▁▁▁▇\n\n\n\n\nmarried\n\n\n1\n\n\n0\n\n\n1\n\n\n0.19\n\n\n0.39\n\n\n0\n\n\n0\n\n\n0.0\n\n\n0.0\n\n\n1\n\n\n▇▁▁▁▂\n\n\n\n\nnodegree\n\n\n0\n\n\n0\n\n\n1\n\n\n0.31\n\n\n0.46\n\n\n0\n\n\n0\n\n\n0.0\n\n\n1.0\n\n\n1\n\n\n▇▁▁▁▃\n\n\n\n\nnodegree\n\n\n1\n\n\n0\n\n\n1\n\n\n0.71\n\n\n0.46\n\n\n0\n\n\n0\n\n\n1.0\n\n\n1.0\n\n\n1\n\n\n▃▁▁▁▇\n\n\n\n\nu74\n\n\n0\n\n\n0\n\n\n1\n\n\n0.09\n\n\n0.28\n\n\n0\n\n\n0\n\n\n0.0\n\n\n0.0\n\n\n1\n\n\n▇▁▁▁▁\n\n\n\n\nu74\n\n\n1\n\n\n0\n\n\n1\n\n\n0.71\n\n\n0.46\n\n\n0\n\n\n0\n\n\n1.0\n\n\n1.0\n\n\n1\n\n\n▃▁▁▁▇\n\n\n\n\nu75\n\n\n0\n\n\n0\n\n\n1\n\n\n0.10\n\n\n0.30\n\n\n0\n\n\n0\n\n\n0.0\n\n\n0.0\n\n\n1\n\n\n▇▁▁▁▁\n\n\n\n\nu75\n\n\n1\n\n\n0\n\n\n1\n\n\n0.60\n\n\n0.49\n\n\n0\n\n\n0\n\n\n1.0\n\n\n1.0\n\n\n1\n\n\n▅▁▁▁▇\n\n\n\n\n\n\n\nA key takeaway is that the experimental data is reasonably balanced across treated and control units (not surprisingly) but the observational data is not."
  },
  {
    "objectID": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#quantile-treatment-effect-estimation-in-experimental-data",
    "href": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#quantile-treatment-effect-estimation-in-experimental-data",
    "title": "Quantile Treatment Effects",
    "section": "Quantile Treatment Effect Estimation in Experimental Data",
    "text": "Quantile Treatment Effect Estimation in Experimental Data\nOur first exercise will evaluate the effect of the training program using the experimental data. Because we have a randomized experiment, we can simply compare the treated and untreated outcomes in the post-treatment period.\nLet’s first take a look at the distribution of outcomes by group:\n\n\n\n\n\n\nFigure 1: Histogram of Earnings Outcome in Experimental Data, by Treatment Status and Year\n\n\n\n\n\nWe next take a difference in means to obtain an estimate of the average treatment effect on the treated:\n\nlalonde.exp %&gt;% \n  group_by(treat) %&gt;% \n  summarise(mean_outcome = mean(re78)) %&gt;% \n  mutate(att = mean_outcome - lag(mean_outcome)) %&gt;% \n  kable(digits = 2) %&gt;% kable_styling()\n\n\n\n\ntreat\nmean_outcome\natt\n\n\n\n\n0\n4554.8\nNA\n\n\n1\n6349.1\n1794.3\n\n\n\n\n\n\n\nSuppose we wanted to estimate treatment effects at different quantiles of the outcome distribution. To get a sense of where these effects may occur, we can plot the empirical cumulative distribution function (eCDF) for each group:\n\n\n\n\n\n\nFigure 2: Empirical CDF of Outcome in Experimental Data, by Treatment Status and Year\n\n\n\n\n\nWe see here that the two distributions have separated a bit, though there are regions where they essentially overlap. Just shy of the 25th percentile, for example, the two distributions overlap at around $0 in income; there doesn’t appear to be any change in earnings for treated individuals within the bottom quarter of the income distribution.\n\nEstimating Quantile Treatment Effects\nSuppose we wanted to estimate the difference at some quantile of interest (e.g., the 25th percentile or the 75th percentile). How would we go about doing this? Essentially, for a selected quantile (e.g., 75th percentile), we need to measure the horizontal distance between the two curves in Figure 2.\nTo build up an estimator that we can apply in our data, we first need to introduce some notation.\nDefine \\(F_{Y(W)wt}(y)\\) as the potential outcome distribution function for an individual receiving treatment \\(W \\in 0,1\\), and who is observed to be in treatment group \\(w\\) at time \\(t\\).\nFor example:\n\n\\(F_{Y(1)11}(y)\\) is the distribution function for the outcome distribution under treatment for treated individuals in the post-treatment period.\n\\(F_{Y(0)01}(y)\\) is the distribution function for the outcome distribution under non-treatment for non-treated individuals in the post-treatment period.\n\nWe can supply each of these distribution functions a value of the outcome \\(y\\), and it will tell us at what quantile \\(\\tau\\) in the distribution that value of \\(y\\) maps to. For example, plugging in a value of $2,000 might tell us that this value is at the 50th percentile (median) of a given outcome distribution.\nStepping back from the notation a bit, note that these two functions are essentially what we already plotted in Figure 2 above. How do we calculate these functions in R?\nFortunately, base R has a defined function (ecdf()) that allows us to do this easily. Let’s define empirical distribution functions for the treated and untreated groups in the experimental Lalonde data:\n\ny11 &lt;- lalonde.exp %&gt;% filter(treat==1) %&gt;% pull(re78)\ny01 &lt;- lalonde.exp %&gt;% filter(treat==0) %&gt;% pull(re78)\n\neF11 &lt;- ecdf(y11)\neF01 &lt;- ecdf(y01)\n             \neF11(2000)\n\n[1] 0.38919\n\neF01(2000)\n\n[1] 0.45\n\n\nWe see in the calculations above that earnings of $2,000 would place you at the 39th percentile of the treated group’s post-intervention outcome distribution, and at the 45th percentile of the untreated groups distribution.\nWe next need to go one step further, however, and define the inverse of the above. That is, we need a function that, provided a quantile value \\(\\tau\\), returns the outcome value that maps to that quantile in the distribution. This is fundamentally what we’ll be working with to estimate treatment effects because we want to know what the effect of the program is at the 75th percentile, not for someone with $2,000 in earnings.\nThe concept described above is known as the inverse distribution function, and in our notation will be defined as \\(F^{-1}_{Y(W)wt}(\\tau)\\). That is, we supply this function with a quantile \\(\\tau\\), and it returns the value associated with the outcome distribution at that particular quantile.\nFrom a practical sense, calculating the inverse CDF in R is straightforward, however there is not a base function like ecdf() we can draw from. Fortunately, the package edfun provides us with an easy way to calculate it.\n\ninvF11 &lt;- edfun::edfun(y11)$qfun\ninvF01 &lt;- edfun::edfun(y01)$qfun\n\ninvF11(0.75)\n\n[1] 9631.9\n\ninvF01(0.75)\n\n[1] 7284.4\n\n\nIn the calculated values above, an earnings value of 9632 would place you at the 75th percentile of the treated group’s post-intervention outcome distribution. Similarly, an earnings value of 7284 would place you at the 75th percentile of the untreated group’s post-intervention outcome distribution.\nWe’re now ready to estimate quantile treatment effects. Formally, the quantile treatment effect on the treated (QTT) at \\(\\tau\\) is obtained as the difference in the inverse distribution function for the treated group in the post period under treatment and under non-treatment:\n\\[\n\\Delta_{\\tau} = F^{-1}_{Y(1)11}(\\tau) - F^{-1}_{Y(0)11}(\\tau)\n\\]\nHowever, as usual the second quantity is an unobserved counterfactual. Given random assignment, we can estimate QTTs using the inverse distribution function from the untreated group.\n\\[\n\\Delta_{\\tau} = F^{-1}_{Y(1)11}(\\tau) - F^{-1}_{Y(0)01}(\\tau)\n\\] So what is QTT(0.75) in the Lalonde experimental data?\n\ninvF11(0.75) - invF01(0.75)\n\n[1] 2347.5\n\n\nThere it is; we’ve calculated a quantile treatment effect in our experimental data!\nIn practice we do not need to go through the above process by hand each time. We can actually just plug our data into the qte package’s functions to obtain estimates and standard errors:\n\natt_experimental &lt;- \n  ci.qtet(re78 ~ treat, \n    data=lalonde.exp, \n    probs=seq(0.05,0.95,0.05), \n    se=T, \n    iters=10)\nres_att_experimental &lt;- summary(att_experimental)\n\n\n\nNote that the qte package computes standard errors via a bootstrap method.\nYou’ll see in Figure 3 that the estimated QTT for the 75th percentile is exactly the same as we calculated above.\n\n\n\n\n\n\n\n\nPercentile\n\n\nTreatment Effect\n\n\nStandard Error\n\n\n\n\n\n\nAvg\n\n\n1794.3\n\n\n525.6\n\n\n\n\n95%\n\n\n3979.6\n\n\n3116.4\n\n\n\n\n90%\n\n\n3239.6\n\n\n2153.2\n\n\n\n\n85%\n\n\n2178.3\n\n\n1101.2\n\n\n\n\n80%\n\n\n2278.1\n\n\n837.0\n\n\n\n\n75%\n\n\n2347.5\n\n\n815.9\n\n\n\n\n70%\n\n\n1795.1\n\n\n987.8\n\n\n\n\n65%\n\n\n2115.0\n\n\n821.5\n\n\n\n\n60%\n\n\n1466.5\n\n\n507.0\n\n\n\n\n55%\n\n\n1181.5\n\n\n635.4\n\n\n\n\n50%\n\n\n1123.5\n\n\n821.5\n\n\n\n\n45%\n\n\n1396.1\n\n\n756.2\n\n\n\n\n40%\n\n\n1177.7\n\n\n909.5\n\n\n\n\n35%\n\n\n1451.5\n\n\n450.4\n\n\n\n\n30%\n\n\n846.4\n\n\n262.1\n\n\n\n\n25%\n\n\n338.6\n\n\n267.6\n\n\n\n\n20%\n\n\n0.0\n\n\n153.4\n\n\n\n\n15%\n\n\n0.0\n\n\n0.0\n\n\n\n\n10%\n\n\n0.0\n\n\n0.0\n\n\n\n\n5%\n\n\n0.0\n\n\n0.0\n\n\n\n\n\nFigure 3: Average and Quantile Treatment Effects for the Experimental Lalonde Data\n\n\n\n\n\n\n\n\nFigure 4: Plot of Quantile Treatment Effects as Estimated Using qte Package"
  },
  {
    "objectID": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#quantile-treatment-effect-estimation-in-observational-data",
    "href": "blog/drafts/quantile-treatment-effects/quantile-treatment-effects.html#quantile-treatment-effect-estimation-in-observational-data",
    "title": "Quantile Treatment Effects",
    "section": "Quantile Treatment Effect Estimation in Observational Data",
    "text": "Quantile Treatment Effect Estimation in Observational Data\nNow suppose we do not have the luxury of experimental data, but instead have a treated group and untreated group drawn from observational data. How can we identify and estimate quantile treatment effects?\nThere are a number of approaches we can take here:\n\nEstimate bounds on the quantile treatment effect distribution.\nEstimate QTTs under an assumption of conditional independence, i.e., unconfoudnendess.\nEstimate QTTs using a quantile difference-in-differences (qDID) estimator.\nEstimate QTTs using a changes-in-changes (CiC) estimator (Athey and Imbens 2006).\n\nPartial identification via estimating bounds (#1) will be a focus of our class next week, so we’ll set it aside for now – though note that the qte package can estimate bounds based on the method in Fan and Yu (2012).\nSimilarly, if we think we have enough measured data satisfy an unconfoundedness assumption (analogous to what we do with propensity scores ore matching) we can also estimate (#2) using the qte package.\nOur focus for today will be on approaches (#3) and especially (#4). We’ll also discuss how we can incorporate covariates to improve identifcation of quantile treatment effects.\n\nIntuition for qDID and CiC\nEssentially, we’re going to lean on similar intuition for identifying QTTs as we do for identifying average treatment effects on the treated using difference-in-differences. In the case of quantile effects, however, we will be thinking in terms of how the distribution functions change in an untreated comparison group. We’ll then use these changes as a “stand-in” for the counterfactual change that would have occurred in the treated group absent the treatment.\nThe difference between qDID and CiC essentially boils down to how we think about the counterfactual. Let’s envision two scenarios with observational data for a job training program under which the treatment group is selected predominantly among unemployed individuals with low earnings:\n\nThe pre-intervention earnings at the 50th percentile in the treatment group are $0, and at the 50th percentile of the untreated group, pre-intervention earnings are $5,000. In the post-intervention period, the untreated group’s earnings increase to $6,000 at the 50th percentile, while the treated group’s earnings rise to $3000 at the 50th percentile.\nThe pre-intervention earnings at the 50th percentile in the treatment group are $0, and $0 in earnings corresponds to the 10th percentile of the untreated group’s pre-intervention earnings distribution. In the post period, the untreated group’s earnings remain at $0 at the 10th percentile, while earnings at the 50th percentile in the treated group rise to $3,000.\n\nWhat counterfactual should we use for the treated group? The $1,000 rise in earnings observed at the 50th percentile in the untreated group, even though the baseline value at the 50th percentile was very different than the treated group’s median value? Or, is a better estimate of the counterfactual the experience of $0 earners at the 10th percentile in the untreated group? In short, the former assumption is used for qDID, while the latter assumption is used for CiC.\n\n\nEstimating qDID and CiC\nWith the intuition for both approaches solidified, let’s now construct quantile treatment effect estimates in the observational Lalonde data using each.\nFirst, let’s pull out the outcome values for treated and control groups in the pre (1975) and post-intervention (1978) periods:\n\ny00 &lt;- lalonde.psid %&gt;% \n  filter(treat==0) %&gt;% \n  pull(re75)\n\ny10 &lt;- lalonde.psid %&gt;% \n  filter(treat==1) %&gt;% \n  pull(re75)\n\ny01 &lt;- lalonde.psid %&gt;% \n  filter(treat==0) %&gt;% \n  pull(re78)\n\ny11 &lt;- lalonde.psid %&gt;% \n  filter(treat==1) %&gt;% \n  pull(re78)\n\nWe’ll next define eCDFs and inverse eCDFs for each:\n\neF00 &lt;- ecdf(y00)\ninvF00 &lt;- edfun::edfun(y00)$qfun\n\neF10 &lt;- ecdf(y10)\ninvF10 &lt;- edfun::edfun(y10)$qfun\n\neF01 &lt;- ecdf(y01)\ninvF01 &lt;- edfun::edfun(y01)$qfun\n\neF11 &lt;- ecdf(y11)\ninvF11 &lt;- edfun::edfun(y11)$qfun\n\nThe next thing we need to do is to define a quantile of interest. Let’s estimate for the 75th percentile.\n\nq = 0.75\n\nNext, let’s estimate the observed change in the treated group at the 75th percentile:\n\ninvF11(q)\n\n[1] 9631.9\n\ninvF10(q)\n\n[1] 1809\n\nchange_treated &lt;- invF11(q) - invF10(q)\nchange_treated\n\n[1] 7822.9\n\n\nSo we observe earnings going up by 7823. But what would have happened counterfactually? For this we will appeal to the untreated group’s experience.\nFirst, let’s estimate what change happens at the 75th percentile of the untreated group’s earnings distribution:\n\ninvF01(q)\n\n[1] 29510\n\ninvF00(q)\n\n[1] 26470\n\ncfx_untreated_q75 &lt;- invF01(q) - invF00(q)\ncfx_untreated_q75 \n\n[1] 3040.2\n\n\nTo construct a qDID estimate, we’d simply net out this counterfactual from the observed change in the treated group:\n\nqtt_qDID &lt;- change_treated - cfx_untreated_q75 \nqtt_qDID \n\n[1] 4782.7\n\n\nUsing qDID, we would estimate that the job trainings program increased earnings by $4783.\nNotice, however, that there is a huge difference in earnings at the 75th percentile of the treated group’s pre-intervention earnings distribution ($1809), and the untreated group’s ($26470).\nAs an alternative, we can adopt a Changes-in-Changes model. Recall from above that this model requires an additional step: we must map the earnings value of the treated group at the 75th percentile to the corresponding quantile in the untreated groups pre-intervention earnings distribution. We then use the change in earnings at this different quantile as an estimate of the counterfactual:\n\n# Earnings at the qth quantile of the treated group\ny_q &lt;- invF10(q)\ny_q\n\n[1] 1809\n\n# Find what quantile this maps to in the untreated group's distribution. \nq_star &lt;- eF00(y_q)\nq_star\n\n[1] 0.11365\n\ncfx_CiC &lt;- invF01(q_star) - y_q\ncfx_CiC\n\n[1] -1809\n\nchange_treated &lt;- invF11(q) - invF10(q)\nchange_treated\n\n[1] 7822.9\n\nqtt_CiC &lt;- change_treated - cfx_CiC\nqtt_CiC\n\n[1] 9631.9\n\n\nWhile the above exercises went through estimating a changes-in-changes value by hand, the formal estimator is a bit simpler due to some cancelling out of terms. Specifically, the counterfactual CDF is given by\n\\[\n\\hat F_{Y(0),11}(y) = F_{y,01}(F_{y,00}^{-1}(F_{y,10}(y)))\n\\] where F_{y,00} is the observed CDF for the untreated group in the pre-period, F_{y,10} is the observed CDF for the treated group in the pre period, etc.\nEquivalently, we can estimate the inverse CDF of the counterfactual as:\n\\[\n\\hat F_{Y(0),11}^{-1}(\\tau) = F_{y,01}^{-1}(F_{y,00}(F^{-1}_{y,10}(\\tau)))\n\\] And the treatement effect estimate is given by:\n\\[\n\\Delta_{\\tau}^{CiC} = F_{Y(1),11}^{-1}(\\tau)- \\hat F_{Y(0),11}^{-1}(\\tau)\n\\] Implemented for our example, the QTT for the 75th percentile is\n\ninvF11(q) - invF01(eF00(invF10(q)))\n\n[1] 9631.9\n\n\nwhich is identical to the “by hand” estimate we calculated above.\nAgain, the qte package in R will calculate changes-in-changes estimates for you, along with standard errors. The estimates are slightly different here, owing to some “lumpiness” in the eCDFs and slight differnces in calculating the values at quantiles in the data. However, the QTTs are quite close to what we estimated above:\n\n\n\nThere is also a Stata implementation of the Changes-in-Changes estimator, which can be found here.\n\ndf &lt;- \n  lalonde.psid %&gt;% \n  select(id,re75,re78,treat) %&gt;% \n  gather(year,y,-id,-treat) %&gt;% \n  tibble() %&gt;% \n  mutate(year = ifelse(year==\"re75\",1975,1978))\n\nest_CiC &lt;- CiC(y ~ treat, t = 1978, tmin1=1975, \n               tname = \"year\", idname = \"id\",\n               data = df, probs = c(q), \n               se = TRUE, panel = F)\nsummary(est_CiC)\n\n\nQuantile Treatment Effect:\n        \ntau QTE Std. Error\n0.75    9643.00 1086.24\n\nAverage Treatment Effect:   5089.64\n     Std. Error:        770.35"
  },
  {
    "objectID": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html",
    "href": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html",
    "title": "Bounds Under Exogenous Treatment Assignment",
    "section": "",
    "text": "Code\nparams &lt;- \n  list(\n    N = 2e3,\n    sigma_sq_X = 1.0,\n    sigma_sq_epsilon = 0.3,\n    delta = 0.5\n  )\nparams &lt;- with(params,\n               modifyList(params,list(\n                 r_squared = 1 - sigma_sq_epsilon,\n                 beta = sqrt(1 - sigma_sq_epsilon),\n                 Sigma = matrix(c(sigma_sq_X,0,0,sigma_sq_epsilon),\n                                byrow=TRUE, nrow = 2, ncol = 2))))\n\ngen_data &lt;- function(params) {\n  with(params, \n       mvrnorm(n = N, mu = c(0,0), Sigma = Sigma)) %&gt;% \n    data.frame() %&gt;% \n    as_tibble() %&gt;% \n    set_names(c(\"X\",\"epsilon\")) %&gt;% \n    mutate(Y_i0 = params$beta * X + epsilon) %&gt;% \n    mutate(Y_i1 = Y_i0 + params$delta) %&gt;% \n    mutate(random = runif(nrow(.))) %&gt;% \n    mutate(D = as.integer(row_number()&lt;=(params$N)/2)) %&gt;% \n    arrange(random) %&gt;% \n    select(-random) %&gt;% \n    mutate(Y = D * Y_i1 + (1 - D) * Y_i0) %&gt;% \n    select(Y,D,X)\n}\n\nset.seed(123)\ndf &lt;- params %&gt;% gen_data()\n\ndf %&gt;% head(n=10) %&gt;% kable() %&gt;% \n    kable_styling()\n\n\n\n\nTable 1: First 10 rows of data\n\n\nY\nD\nX\n\n\n\n\n0.636525\n0\n1.136893\n\n\n-1.017545\n0\n-1.143835\n\n\n-2.660006\n0\n-1.769366\n\n\n-0.234232\n0\n-1.314321\n\n\n-1.052531\n1\n-1.236676\n\n\n-0.082499\n0\n-1.321069\n\n\n1.284970\n1\n1.516068\n\n\n0.271550\n1\n0.246692\n\n\n-0.690019\n0\n-1.433008\n\n\n-0.084229\n0\n-0.229395"
  },
  {
    "objectID": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html#lower-bounds",
    "href": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html#lower-bounds",
    "title": "Bounds Under Exogenous Treatment Assignment",
    "section": "Lower Bounds",
    "text": "Lower Bounds"
  },
  {
    "objectID": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html#upper-bounds",
    "href": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html#upper-bounds",
    "title": "Bounds Under Exogenous Treatment Assignment",
    "section": "Upper Bounds",
    "text": "Upper Bounds\n\nCalculate the lower bound values\nThis code turns the lower bound quantile values into an empirical CDF object.\nCalculate the upper bound values\nThis code turns the lower bound quantile values into an empirical CDF object.\n\nNow let’s look at the bounds for the quantile of the Treated Effect on the Treated (QoTT), i.e., the bounds on various quantiles of the treatment effect distribution.\n\n\nCode\nqwdu &lt;- quantile(F.wd.l, tau, type=1)\nqwdl &lt;- quantile(F.wd.u, tau, type=1)\n\ndf_wd &lt;- \n  data.frame(tau = c(tau,tau), bound = c(qwdu,qwdl), type=c(rep(\"upper\",length(qwdu)),rep(\"lower\",length(qwdl)))) %&gt;% \n  mutate(method =\"Worst-Case\\n[Williamson-Downs (1990)]\") %&gt;% \n  mutate(label = ifelse(type==\"upper\",\"Worst-Case\\n[Williamson-Downs (1990)]\",\"\")) \n\ncol_scheme &lt;- c(\"#FF5733\" ) # Williamson and Downs\n\ndf_wd %&gt;% \n  mutate(type = paste0(type,method)) %&gt;% \n  ggplot(aes(x = tau, y = bound, group = type, colour = method)) + \n  geom_line(lwd=1.25) + \n  geom_hline(aes(yintercept = params$delta), colour = \"darkred\",lwd=1.25) + \n  scale_color_manual(values = col_scheme) + \n  geom_dl(method = list(\"last.points\",hjust=1),aes(label = label)) +\n  scale_x_continuous(breaks = seq(0,1,0.1)) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\",x = 0.5, y = params$delta, label = \"True Treatment Effect\",vjust=-1,colour = \"darkred\") + \n  scale_y_continuous(limits = c(-5,5),breaks = seq(-5,5,1))\n\n\n\n\n\nFigure 1: Worst-Case (Williamson and Downs [1990]) Bounds"
  },
  {
    "objectID": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html#incorporating-covariates",
    "href": "blog/drafts/partial-ID-experimental/partial-ID-experimental.html#incorporating-covariates",
    "title": "Bounds Under Exogenous Treatment Assignment",
    "section": "Incorporating Covariates",
    "text": "Incorporating Covariates\n\\(F_{\\Delta \\mid Y(0), X}^L(t \\mid Y(0), X):= \\begin{cases}0, & Y(0)+t&lt;\\tilde{Y}(1 \\mid X), \\\\ \\frac{F_{1 \\mid X}(Y(0)+t \\mid X)-F_{0 \\mid X}(Y(0) \\mid X)}{1-F_{0 \\mid X}(Y(0) \\mid X)}, & Y(0)+t \\geq \\tilde{Y}(1 \\mid X),\\end{cases}\\)\n\\(F_{\\Delta \\mid Y(0), X}^U(t \\mid Y(0), X):= \\begin{cases}\\frac{F_{1 \\mid X}(Y(0)+t \\mid X)}{F_{0 \\mid X}(Y(0) \\mid X)}, & Y(0)+t \\leq \\tilde{Y}(1 \\mid X) \\\\ 1, & Y(0)+t \\geq \\tilde{Y}(1 \\mid X)\\end{cases}\\)\n\\(\\begin{aligned} & F_{\\Delta \\mid Y(d)}^L(t \\mid Y(d))=E\\left[F_{\\Delta \\mid Y(d), X}^L(t \\mid Y(d), X) \\mid Y(d)\\right] \\\\ & F_{\\Delta \\mid Y(d)}^U(t \\mid Y(d))=E\\left[F_{\\Delta \\mid Y(d), X}^U(t \\mid Y(d), X) \\mid Y(d)\\right]\\end{aligned}\\)\n\\(\\begin{aligned} & \\Delta^L(Y(d))=\\int t d F_{\\Delta \\mid Y(d)}^U(t \\mid Y(d)) \\\\ & \\Delta^U(Y(d))=\\int t d F_{\\Delta \\mid Y(d)}^L(t \\mid Y(d))\\end{aligned}\\)\n\\(\\begin{aligned} & \\hat{F}_{\\Delta \\mid 0, X}^L\\left(t \\mid Y_j(0), X_j\\right):=\\max \\left\\{0, \\frac{\\hat{F}_{1 \\mid X}\\left(Y_j(0)+t \\mid X_j\\right)-\\hat{F}_{0 \\mid X}\\left(Y_j(0) \\mid X_j\\right)}{1-\\hat{F}_{0 \\mid X}\\left(Y_j(0) \\mid X_j\\right)}\\right\\} \\\\ & \\hat{F}_{\\Delta \\mid 0, X}^U\\left(t \\mid Y_j(0), X_j\\right):=\\min \\left\\{1, \\frac{\\hat{F}_{1 \\mid X}\\left(Y_j(0)+t \\mid X_j\\right)}{\\hat{F}_{0 \\mid X}\\left(Y_j(0) \\mid X_j\\right)}\\right\\} .\\end{aligned}\\)\n\ni1 &lt;-  # index of treated observations\n  df %&gt;% \n  mutate(i = row_number()) %&gt;% \n  filter(D==1) %&gt;% \n  pull(i)\n\ni0 &lt;-  # index of untreated observations\n  df %&gt;% \n  mutate(i = row_number()) %&gt;% \n  filter(D==0) %&gt;% \n  pull(i)\n\nX1 &lt;- # X values of treated observations\n  df %&gt;%\n  filter(D==1) %&gt;%\n  pull(X)\n\nX0 &lt;- # X values of untreated observations\n  df %&gt;% \n  filter(D==0) %&gt;% \n  pull(X)\n\nWe next create conditional CDFs, with a different CDF constructed for each untreated group \\(X\\) value.\nThe short way uses parametric quantile regression (rq()). This specification assumes a model that is linear in parameters.\n\ntaus0 &lt;- sort(ecdf(y0)(y0))\nquantreg_fit &lt;- quantreg::rq(Y ~ X, tau = taus0, data = df %&gt;% filter(D==0))\npvals &lt;- predict(quantreg_fit,newdata = data.frame(X=X0), stepfun=TRUE)\nhatF0X &lt;- pvals %&gt;% map(~(.x(taus0))) %&gt;% \n  map(~(BMisc::makeDist(.x,taus0)))\n\n\ntaus1 &lt;- sort(ecdf(y1)(y1))\nquantreg_fit &lt;- quantreg::rq(Y ~ X, tau = taus0, data = df %&gt;% filter(D==1))\npvals &lt;- predict(quantreg_fit,newdata = data.frame(X=X0), stepfun=TRUE)\nhatF1X &lt;- pvals %&gt;% map(~(.x(taus0))) %&gt;% \n  map(~(BMisc::makeDist(.x,taus0)))\n\nA (much) longer, but nonparametric approach, is to fit a nonparametric regression. In this example code, we fit a generalized additive model (GAM) with a logit link (for the constructed binary outcome needed to estimate the CDF).Note that the treatment effect bounds calcualted under a GAM vs. a parametric quantile regression approach for the condtiional CDFs are nearly identical in this example—though this will not generally be true!\n\n\nCode\n# This version constructs a full CDF for each X value\npb &lt;- progress_bar$new(total = length(y0))\nhatF0X &lt;-\n  map(X0, ~({\n    # for each untreated observation j\n    xx = .x\n    pb$tick()\n    hatF0X_ &lt;-\n      y_ %&gt;% map_dbl( ~({\n        yy = .x\n        # nonparameterically regress an indicator Y_i &lt;= Y_j on X_i in the untreated subsample\n        IY = as.integer(df$Y[i0] &lt;= yy)\n        X &lt;- X0\n        dat &lt;- cbind.data.frame(IY, X) %&gt;%\n          set_names(c(\"IY\", \"X\"))\n        \n        fit &lt;- gam(IY ~ s(X), data = dat, family = \"binomial\")\n        # construct predicted value\n        predict(fit, newdata = data.frame(X = xx), type = \"response\")\n        \n      })) %&gt;%\n      BMisc::makeDist(y_, .) # make this into an ecdf \n    \n    hatF0X_\n  }))\nwrite_rds(hatF0X,\"posts/extending-the-toolkit-experimental/results/hatF0X.rds\")\n\npb &lt;- progress_bar$new(total = length(X0))\nhatF1X &lt;-\n  map(X0, ~ ({\n    # for each untreated observation j\n    xx = .x\n    pb$tick()\n    hatF1X_ &lt;-\n      y_ %&gt;% map_dbl( ~ ({\n        yy = .x\n        # nonparameterically regress an indicator Y_i &lt;= Y_j on X_i in the treated subsample\n        IY = as.integer(df$Y[i1] &lt;= yy)\n        X &lt;- X1\n        dat &lt;- cbind.data.frame(IY, X) %&gt;%\n          set_names(c(\"IY\", \"X\"))\n        \n        fit &lt;- gam(IY ~ s(X), data = dat, family = \"binomial\")\n        # construct predicted value\n        predict(fit, newdata = data.frame(X = xx), type = \"response\")\n        \n      })) %&gt;%\n      BMisc::makeDist(y_, .) # make this into an ecdf\n    hatF1X_\n  }))\nwrite_rds(hatF1X,\"posts/extending-the-toolkit-experimental/results/hatF1X.rds\")\n\n\nWe next plug these condtiional CDFs into the formulas to obtain the CDF of the treatment effect bounds.\n\n\nCode\npb &lt;- progress_bar$new(total = length(delta_))\nF_l_flX_ &lt;- \n  delta_ %&gt;% map(~{ # outer loop is over possible values of the treatment effect\n    delt &lt;- .x\n    pb$tick()\n      map2_dbl(y0,X0,~({\n        y0_ = .x \n        xx_ = .y\n        i = which(X0==xx_); i\n        F1_fl_tmp &lt;- hatF1X[[i]]\n        F0_fl_tmp &lt;- hatF0X[[i]]\n        (F1_fl_tmp(y0_ + delt)-F0_fl_tmp(y0_))/(1-F0_fl_tmp(y0_))\n      }))  %&gt;% \n      map_dbl(~(max(0,.x,na.rm=TRUE))) %&gt;% \n      mean(.)\n  }) %&gt;% \n  unlist()\n# Make it into a proper eCDF object.\nF.flX.l &lt;- approxfun(delta_,  F_l_flX_, method = \"constant\", yleft = 0, yright = 1, \n                    f = 0, ties = \"ordered\")\nclass(F.flX.l) &lt;- c(\"ecdf\", \"stepfun\", class(F.flX.l))\nassign(\"nobs\", length(delta_), envir = environment(F.flX.l))\n\npb &lt;- progress_bar$new(total = length(delta_))\nF_u_flX_ &lt;- \n  delta_ %&gt;% \n  map(~{ # outer loop is over possible values of the treatment effect\n    delt &lt;- .x\n    pb$tick()\n    \n      map2_dbl(y0,X0,~({\n        y0_ = .x \n        xx_ = .y\n        i = which(X0==xx_); i\n        F1_fl_tmp &lt;- hatF1X[[i]]\n        F0_fl_tmp &lt;- hatF0X[[i]]\n        \n        (F1_fl_tmp(y0_+delt))/(F0_fl_tmp(y0_))\n      }))  %&gt;% \n      map_dbl(~(min(1,.x,na.rm=TRUE))) %&gt;% \n      mean(.)\n  }) %&gt;% \n  unlist()\n# Make it into a proper eCDF object. \nF.flX.u &lt;- approxfun(delta_,  F_u_flX_, method = \"constant\", yleft = 0, yright = 1, \n                    f = 0, ties = \"ordered\")\nclass(F.flX.u) &lt;- c(\"ecdf\", \"stepfun\", class(F.flX.u))\nassign(\"nobs\", length(delta_), envir = environment(F.flX.u))\n\n\nFirst, let’s take a look at bounds on various quantiles of the treatment effect (QoTT):\n\n\nCode\nqflXu &lt;- quantile(F.flX.l, tau, type=1)\nqflXl &lt;- quantile(F.flX.u, tau, type=1)\n\ndf_flX &lt;- \n  data.frame(tau = c(tau,tau), bound = c(qflXu,qflXl), type=c(rep(\"upper\",length(qflXu)),rep(\"lower\",length(qflXl)))) %&gt;% \n  mutate(method = \"Frandsen & Lefgrens (2021) Covariates\") %&gt;% \n  mutate(label = ifelse(type==\"upper\",\"Frandsen & Lefgrens (2021)\\nWith Covariates\",\"\")) \n\n\n\n\nCode\ncol_scheme &lt;- c(\"#0A2E36\" ,# Frandsen & Lefgrens (No X)\n                \"#7DC4CC\", # Frandsen & Lefgrens (X)\n                \"#FF5733\" ) # Williamson and Downs\n\ndf_fl %&gt;% \n  bind_rows(df_flX) %&gt;% \n  bind_rows(df_wd) %&gt;%  \n  mutate(method = factor(method, levels = c(\"Frandsen & Lefgrens (2021)\"     ,       \"Frandsen & Lefgrens (2021) Covariates\", \"Worst-Case\\n[Williamson-Downs (1990)]\"))) %&gt;% \n  mutate(type = paste0(type,method)) %&gt;% \n  ggplot(aes(x = tau, y = bound, group = type, colour = method)) + \n  geom_line(lwd=1.25) + \n  geom_hline(aes(yintercept = params$delta), colour = \"darkred\",lwd=1.25) + \n  scale_color_manual(values=col_scheme) + \n  geom_dl(method = list(\"last.points\",hjust=1),aes(label = label)) +\n  scale_x_continuous(breaks = seq(0,1,0.1)) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\",x = 0.5, y = params$delta, label = \"True Treatment Effect\",vjust=-1,colour = \"darkred\") + \n  scale_y_continuous(limits = c(-5,5),breaks = seq(-5,5,1))\n\n\n\n\n\nFigure 3: Treatment Effect Distribution Bounds with Covariates"
  },
  {
    "objectID": "blog/drafts/quasi-demeaning/quasi-demeaning.html",
    "href": "blog/drafts/quasi-demeaning/quasi-demeaning.html",
    "title": "Quasi-Demeaning: The Relationship Between Fixed and Random Effects",
    "section": "",
    "text": "Assume the following data generation process:\n\\[\nY_{it} = \\mathbf{X}_{i}'\\beta + \\tau D_{it} + U_i +\\epsilon_{it}\n\\] where \\(i\\) indexes individual units and \\(t\\) indexes time, \\(\\mathbf{X}_i\\) are unit-level attributes, \\(D_{it}\\) is a treatment indicator (set to 1 if the observation is treated and in the post-treatment period). Finally, \\(U_i\\) captures unobserved unit-level heterogeneity.\n\n\n\nRandom effects uses an approach called “quasi-demeaning” or “partial pooling.”\nThe random effects model can be represented as:\n\n\\[\n\\begin{align}\n(Y_{it}-\\theta \\bar{Y_i}) = (\\mathbf{X}_{it}-\\theta \\bar{\\mathbf{X}_i})'\\beta + \\tau (D_{it}-\\theta \\bar{D_i})  + (\\epsilon_{it} - \\theta \\bar{\\epsilon_i}) \\quad \\quad \\quad Eq. 9\n\\end{align}\n\\]\nwhere\n\\[\n\\theta = 1 - \\bigg [\\frac{\\sigma^2_\\epsilon}{(\\sigma^2_\\epsilon + T\\sigma^2_u)}\\bigg ]^{1/2}\n\\]\nIn the above, \\(\\sigma_u\\) is the variance of unit-level heterogeneity, and \\(\\sigma_{\\epsilon}\\) is the variance of \\(\\epsilon_{it}\\). And recall \\(T\\) is the total number of repeated unit-level observations in our panel (e.g., the total number of time periods, the total number of patients for a given physician, etc.)\n\nWhen \\(\\theta = 0\\), it just reduces pooled regression.\nWhen \\(\\theta = 1\\), it is equivalent to fixed effects regression. This only isolates variation within units to estimate the regression coefficients (i.e., units are only compared to themselves, which is why the unobserved heterogeneity is accounted for).\nEssentially, when you fit a random effects regression, Stata estimates \\(\\theta\\) and then plugs that estimate it into the above model.\n(When you fit a fixed effects regression, Stata uses the demeaning approach–unless, of course, you manually fit a dummy variable model)\nOften \\(0 &lt; \\theta &lt; 1\\), hence the term “partial-pooling.” That is, the regression draws on both variation “within” units and variation “between” units.\nThe degree to which within and between variation is used depends on the data context.\nRemember, when \\(\\theta=0\\) the random effects regression reduces to pooled OLS. When is this the case?\n\nIf the term \\(T\\sigma^2_u\\) is 0, i.e., \\(\\sigma^2_u=0\\) or there is no variation in individual heterogeneity.\n\nRemember, when \\(\\theta=1\\) the random effects regression reduces to fixed effects. When is this the case?\n\nIf the term \\(T\\sigma^2_u\\) gets super large (more formally, it would need to blast off towards infinity…).\nIf we have a very “large” panel of observations on each unit (i.e., large \\(T\\)) there is sufficient “within” variation and we really don’t need to do any pooling across units."
  },
  {
    "objectID": "blog/drafts/quasi-demeaning/quasi-demeaning.html#estimation-random-vs.-fixed-effects",
    "href": "blog/drafts/quasi-demeaning/quasi-demeaning.html#estimation-random-vs.-fixed-effects",
    "title": "Quasi-Demeaning: The Relationship Between Fixed and Random Effects",
    "section": "",
    "text": "Random effects uses an approach called “quasi-demeaning” or “partial pooling.”\nThe random effects model can be represented as:\n\n\\[\n\\begin{align}\n(Y_{it}-\\theta \\bar{Y_i}) = (\\mathbf{X}_{it}-\\theta \\bar{\\mathbf{X}_i})'\\beta + \\tau (D_{it}-\\theta \\bar{D_i})  + (\\epsilon_{it} - \\theta \\bar{\\epsilon_i}) \\quad \\quad \\quad Eq. 9\n\\end{align}\n\\]\nwhere\n\\[\n\\theta = 1 - \\bigg [\\frac{\\sigma^2_\\epsilon}{(\\sigma^2_\\epsilon + T\\sigma^2_u)}\\bigg ]^{1/2}\n\\]\nIn the above, \\(\\sigma_u\\) is the variance of unit-level heterogeneity, and \\(\\sigma_{\\epsilon}\\) is the variance of \\(\\epsilon_{it}\\). And recall \\(T\\) is the total number of repeated unit-level observations in our panel (e.g., the total number of time periods, the total number of patients for a given physician, etc.)\n\nWhen \\(\\theta = 0\\), it just reduces pooled regression.\nWhen \\(\\theta = 1\\), it is equivalent to fixed effects regression. This only isolates variation within units to estimate the regression coefficients (i.e., units are only compared to themselves, which is why the unobserved heterogeneity is accounted for).\nEssentially, when you fit a random effects regression, Stata estimates \\(\\theta\\) and then plugs that estimate it into the above model.\n(When you fit a fixed effects regression, Stata uses the demeaning approach–unless, of course, you manually fit a dummy variable model)\nOften \\(0 &lt; \\theta &lt; 1\\), hence the term “partial-pooling.” That is, the regression draws on both variation “within” units and variation “between” units.\nThe degree to which within and between variation is used depends on the data context.\nRemember, when \\(\\theta=0\\) the random effects regression reduces to pooled OLS. When is this the case?\n\nIf the term \\(T\\sigma^2_u\\) is 0, i.e., \\(\\sigma^2_u=0\\) or there is no variation in individual heterogeneity.\n\nRemember, when \\(\\theta=1\\) the random effects regression reduces to fixed effects. When is this the case?\n\nIf the term \\(T\\sigma^2_u\\) gets super large (more formally, it would need to blast off towards infinity…).\nIf we have a very “large” panel of observations on each unit (i.e., large \\(T\\)) there is sufficient “within” variation and we really don’t need to do any pooling across units."
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "",
    "text": "Part one in a six-part series on partial identification methods for causal inference and policy decisionmaking.\nSetup R session\nlibrary(tidyverse)\nlibrary(here)\nlibrary(glue)\nlibrary(copula)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggsci)\nlibrary(MASS)\nlibrary(psych)\nlibrary(patchwork)\nlibrary(directlabels)\nlibrary(progress)\nlibrary(latex2exp)\nselect &lt;- dplyr::select\noptions(\"scipen\"=100, \"digits\"=6)\ntheme_set(hrbrthemes::theme_ipsum())\n\nlibrary(tufte)\n# invalidate cache when the tufte version changes\nknitr::opts_chunk$set(cache.extra = packageVersion('tufte'))\noptions(htmltools.dir.version = FALSE)\n\n# knitr::purl(input = \"./partial-id-intro.qmd\",\n#             output = \"./partial-id-intro.r\")"
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#the-cumulative-distribution-function-as-the-unsung-hero-of-causal-inference",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#the-cumulative-distribution-function-as-the-unsung-hero-of-causal-inference",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "The Cumulative Distribution Function as the Unsung Hero of Causal Inference",
    "text": "The Cumulative Distribution Function as the Unsung Hero of Causal Inference"
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#unit-level-treatment-effects",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#unit-level-treatment-effects",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "Unit-Level Treatment Effects",
    "text": "Unit-Level Treatment Effects\nBecause (for now) we observe both potential outcomes, we have all the information we need to define unit-level treatment effects. Let’s define delta as the unit-level difference in potential outcomes:\n\n\n\n\nTable 2: Example Data With Unit-Level Treatment Effects\n\n\npatient\nY_0\nY_1\nD\ndelta\n\n\n\n\nA\n1\n7\n1\n6\n\n\nB\n6\n5\n1\n-1\n\n\nC\n1\n5\n1\n4\n\n\nD\n8\n7\n1\n-1\n\n\nE\n2\n4\n1\n2\n\n\nF\n1\n10\n0\n9\n\n\nG\n10\n1\n0\n-9\n\n\nH\n6\n5\n0\n-1\n\n\nI\n7\n3\n0\n-4\n\n\nJ\n8\n9\n0\n1\n\n\n\n\n\n\n\n\nBecause we can calculate delta for each unit, we can do all sorts of things. For example, we can look at the distribution of delta in our full sample (of 10,000 units) to get a sense of the distribution of the treatment effect. Figure 1 below plots this distribution.\n\n\n\n\n\nFigure 1: Distribution of the Treatment Effect (delta)\n\n\n\n\nWe see from Figure 1 that the treatment effect is positive, on average, but is negative for some individuals. This is relevant, actionable information not captured by the average treatment effect alone!"
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#decision-thresholds-for-policy-evaluation",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#decision-thresholds-for-policy-evaluation",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "Decision Thresholds for Policy Evaluation",
    "text": "Decision Thresholds for Policy Evaluation\nBefore we move on, it’s worth pausing for a minute to consider the evidence thresholds or decision rules that might be relevant to a policymaker. Doing so can help higlight the limitations of standard approaches to policy evaluation, and can also help elucidate additional dimensions of exploration partial identification approaches open up.\nSuppose, for example, that a policymaker faces a decision over whether to expand or continue a treatment or program. What rules might she take into consideration when making this decision?\nIt may be, for example, that the policymaker simply needs the policy or program to be beneficial, on average, in the population. Or, the policymaker may additionally (or solely) want to ensure that the policy does little to no harm—that is, that less than some maximum threshold fraction (\\(\\lambda_1\\)) of the population is made worse off under treatment. Alternatively, the policymaker may wish to ensure that the policy has a minimum treatment effect of at least \\(\\lambda_2\\) for a majority of the population.\nThese are all reasonable and useful decision criteria. **A key challenge is that even under the most ideal experimental designs, we can’t estimate many these quantities of interset—at least, not without some additional (often heroic) assumptions."
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#estimating-the-fraction-harmed-by-treatment",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#estimating-the-fraction-harmed-by-treatment",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "Estimating The Fraction Harmed by Treatment",
    "text": "Estimating The Fraction Harmed by Treatment\nBefore we get to the reasons why we often cannot estimate many of the above parameters of interest, let’s first calculate one of them in our “perfect” data: the fraction of the population harmed by treatment.\nTo estimate this fraction, we can plot the empirical cumulative distribution function (eCDF) of delta among the treated observations: Recall that the CDF for some variable X is the function \\(F_X(x)=\\mathrm{P}(X \\leq x)\\)\n\n\n\n\n\nFigure 2: Cumulative Distribution Function of the Treatment Effect (delta)\n\n\n\n\nThe red-shaded region in Figure 2 shows that 22 percent of treated samples had a negative (unit-level) treatment effect value for delta."
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#the-fundamental-problem-of-causal-inference",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#the-fundamental-problem-of-causal-inference",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "The Fundamental Problem of Causal Inference",
    "text": "The Fundamental Problem of Causal Inference\nWe have seen above that with perfect data, we can estimate that, on average, the treatment is beneficial: it increases survival by 2 years. But we can also calculate that 22 percent of the population is harmed. Armed with this information, a policymaker could weigh the average benefit of the policy against the observation that some people are made worse off.\nA major challenge arises, however, because we almost never observe the perfect data we were able to draw on above.\nWhat is the underlying issue that precludes us from estimating the quantities of interest? The simple reason is the running example above was based on a scenario in which we had full information on potential outcomes for each unit in our population. That is, we observed each unit’s outcome under treatment, and their outcome without treatment. Because of this, we can calculate a unit-level treatment effect estimate delta and plot the CDF of that (Figure 2). However, this is an idealized data scenario that is almost never the case.\nAs many intoductory causal inferences course will teach you, what is more typical is to observe just one realization of the potential outcomes. Among treated units we observe the realized value of \\(Y(1)\\) , and among untreated units we observe the realized value of \\(Y(0)\\). Each unit’s other (counterfactual) potential outcome is therefore missing from observation.\nTable 3 re-casts the data above to show what we would observe in most real-world settings. That is, we only observe values in the column Y_1 for treated observations; and likewise, we only observe values in the column Y_0 for untreated observations. We then collect these observed values in a new column called Y_obs (Y-observed). And because we do not observe both potential outcomes for each unit, we cannot calculate unit-leve values of delta at all.\n\n\n\n\nTable 3: Perfect Doctor Data\n\n\npatient\nD\nY_obs\nY_0\nY_1\ndelta\n\n\n\n\nA\n1\n7\n?\n7\n?\n\n\nB\n1\n5\n?\n5\n?\n\n\nC\n1\n5\n?\n5\n?\n\n\nD\n1\n7\n?\n7\n?\n\n\nE\n1\n4\n?\n4\n?\n\n\nF\n0\n1\n1\n?\n?\n\n\nG\n0\n10\n10\n?\n?\n\n\nH\n0\n6\n6\n?\n?\n\n\nI\n0\n7\n7\n?\n?\n\n\nJ\n0\n8\n8\n?\n?"
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#scale-dependence-of-common-estimators",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#scale-dependence-of-common-estimators",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "Scale Dependence of Common Estimators",
    "text": "Scale Dependence of Common Estimators\nIn addition to expanding the toolkit of policy- and decision-relevant parameters that can be estimated, analyzing policy impact using the CDF also helps highlight how we can test & address another issue: scale-dependence of common estimators. This is an area covered in detail in Roth and Sant’Anna (2023), and in our my own recent work on Difference-in-Differences for categorical outcomes (Graves et al. 2022).\n\nIn short, for parallel trends assumptions to be robust to functional form, need to identify the entire distribution of counterfactual outcomes for the treated group.\n\nparallel trends assumption is invariant to transformations if and only if a “parallel trends”-type assumption holds for the entire cumulative distribution function (CDF) of untreated potential outcomes\n\nThere are three cases in which this condition holds:\n\nIf the distribution of potential outcomes is the same for both groups, as occurs under random assignment of treatment.\nIf the potential outcome distributions for each group are stable over time.\nA hybrid of the first two cases in which the population is a mixture of a sub-population that is effectively randomized between treatment and control and another sub-population that has non-random treatment status but stable potential outcome distributions across time.\n\n\ndemonstrate that parallel trends is either a functional form restriction or a combination of unconfoundedness and stationary assumptions"
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#how-can-we-recover-useful-treatment-effect-parameters",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#how-can-we-recover-useful-treatment-effect-parameters",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "How Can We Recover Useful Treatment Effect Parameters?",
    "text": "How Can We Recover Useful Treatment Effect Parameters?\nIn this example, we have simulated data under a hypothetical experimental design (i.e., treatment D is exogenously assigned and is therefore independent of the potential outcomes).\nWhat can we point identify using the marginal distributions of the outcome (by treatment status) under this experimental design?\nFirst, we can identify Average Treatment Effect on the Treated (ATT):The ATT is given by \\(ATT = E[Y_{1t}-Y_{0t}|D=1]\\)\n\n\n\n\nTable 4: Mean Outocomes by Treatment Status, and Average Treatment Effect Estimate\n\n\nUntreated\nTreated\nhat_delta\n\n\n\n\n5.584\n7.619\n2.035\n\n\n\n\n\n\n\n\nThis is getting us somewhere. We learn that, on average, the treatment was beneficial in extending survival—but crutially, we can’t rule out that it harmed some units in the population.\nWe can also define the Quantile Treatment Effect on the Treated (QTT):\n\\[\nQ T T(\\tau)=F_{Y_{1 t} \\mid D=1}^{-1}(\\tau)-F_{Y_{0 t} \\mid D=1}^{-1}(\\tau)\n\\]\n\n\n\n\n\nFigure 3: Quantile Treatment Effects\n\n\n\n\nIs it possible to recover useful and actionable information on this question? Yes, it is! It turns out that we can’t point identify some quantities of interest without strong assumptions, but we can set identify them with minimal assumptions. In other words, without having to go do extraorinarily lenghts in terms of assumptions, we can put upper and lower bounds on the fraction of the population harmed by treatment, or on the fraction of the population with treatment effect value greater than or less than some critical value \\(lambda\\)."
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#bounds-on-the-distribution-of-the-treatment-effect",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#bounds-on-the-distribution-of-the-treatment-effect",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "Bounds on the Distribution of the Treatment Effect",
    "text": "Bounds on the Distribution of the Treatment Effect\nWe’ll first construct worst-case bounds based on techniques covered in Abbring and Heckman (2007), Williamson and Downs (1990) and Frank, Nelsen, and Schweizer (1987).\n\nSharp bounds on the joint distribution of the potential outcomes with identiÖed marginals are given by the Frechet-Hoe§ding lower and upper bound distributions, see Heckman and Smith (1993), Heckman, Smith, and Clements (1997), and Manski (1997b) for their applications in program evaluation.\n\n\n\n\n\n\nFigure 4: Bounds on the Cumulative Distribution Function of the Treatment Effect (delta)\n\n\n\n\n\n\nCode\n#col_scheme &lt;- c(\"#FF5733\" ) # Williamson and Downs\n\ndf_wd  %&gt;% \n  mutate(method = factor(method, levels = c(\"Frandsen & Lefgrens (2021)\"     ,       \"Frandsen & Lefgrens (2021) Covariates\", \"Worst-Case\\n[Williamson-Downs (1990)]\"))) %&gt;% \n  mutate(type = paste0(type,method)) %&gt;% \n  ggplot(aes(x = tau, y = bound, group = type, colour = method)) + \n  geom_line(lwd=1.5,colour = \"#FF5733\") + \n  geom_dl(method = list(\"last.points\",hjust=1),aes(label = label)) +\n  scale_x_continuous(breaks = seq(0,1,0.1)) +\n  theme(legend.position = \"none\")  +\n  scale_colour_manual(values = col_scheme[3]) +\n  labs(x = TeX(\"$$\\\\tau$$\"),y=TeX(\"QoTT($$\\\\tau$$)\"))\n  # geom_point(data =  tibble(tau = taus, qtt = QTT, type = \"True Values\", method = \"True Values\"), aes(x = tau, y = qtt ),\n  #            col=\"darkred\") \n\n\n\n\n\nFigure 5: Worst-Case Treatment Effect Distribution Bounds\n\n\n\n\nWe’ll next construct bounds based on Frandsen and Lefgren (2021).\n\n\n\n\n\nFigure 6: Frandsen-Lefgren (2021) Bounds on the Cumulative Distribution Function of the Treatment Effect (delta)\n\n\n\n\n\n\n\n\n\nFigure 7: Worst-Case Bounds on the Average Treatment Effect on the Treated, by Method\n\n\n\n\n\n\nCode\n#col_scheme &lt;- c(\"#FF5733\" ) # Williamson and Downs\n\ndf_wd  %&gt;% \n  bind_rows(df_fl) %&gt;% \n  mutate(method = factor(method, levels = c(\"Frandsen & Lefgrens (2021)\"     ,       \"Frandsen & Lefgrens (2021) Covariates\", \"Worst-Case\\n[Williamson-Downs (1990)]\"))) %&gt;% \n  mutate(type = paste0(type,method)) %&gt;% \n  ggplot(aes(x = tau, y = bound, group = type)) + \n  geom_line(lwd=1.25,aes(colour = method)) + \n  scale_color_manual(values=col_scheme[c(1,3)]) + \n  geom_dl(method = list(\"last.points\",hjust=1),aes(label = label)) +\n  scale_x_continuous(breaks = seq(0,1,0.1)) +\n  theme(legend.position = \"none\")  +\n   labs(x = TeX(\"$$\\\\tau$$\"),y=TeX(\"QoTT($$\\\\tau$$)\"))\n\n\n\n\n\nFigure 8: Fransden-Lefgren Treatment Effect Distribution Bounds\n\n\n\n\nLet’s get it even tighter with covariates. This is an approach based on Frandsen and Lefgren (2021).\n\n\n\n\n\nFigure 9: Frandsen-Lefgren (2021) Bounds on the Cumulative Distribution Function of the Treatment Effect, With Covariates\n\n\n\n\n\n\n\n\n\nFigure 10: Worst-Case Bounds on the Average Treatment Effect on the Treated, by Method\n\n\n\n\n\n\nCode\ndf_wd  %&gt;% \n  bind_rows(df_fl) %&gt;% \n  bind_rows(df_flX) %&gt;% \n  mutate(method = factor(method, levels = c(\"Frandsen & Lefgrens (2021)\"     ,       \"Frandsen & Lefgrens (2021)\\nWith Covariates\", \"Worst-Case\\n[Williamson-Downs (1990)]\"))) %&gt;% \n  mutate(type = paste0(type,method)) %&gt;% \n  ggplot(aes(x = tau, y = bound, group = type)) + \n  geom_line(lwd=1.25,aes(colour = method)) + \n  scale_color_manual(values=col_scheme) + \n  geom_dl(method = list(\"last.points\",hjust=1),aes(label = label,colour = method)) +\n  scale_x_continuous(breaks = seq(0,1,0.1)) +\n  theme(legend.position = \"none\")  +\n  labs(x = TeX(\"$$\\\\tau$$\"),y=TeX(\"QoTT($$\\\\tau$$)\"))\n  #annotate(\"text\",x = 0.5, y = 2, label = \"True Treatment Effect\",vjust=-1,colour = \"darkred\") \n\n\n\n\n\nFigure 11: Fransden-Lefgren Treatment Effect Distribution Bounds With Covariates"
  },
  {
    "objectID": "blog/drafts/partial-id-intro/partial-id-intro.html#bounds-on-the-average-treatment-effect",
    "href": "blog/drafts/partial-id-intro/partial-id-intro.html#bounds-on-the-average-treatment-effect",
    "title": "Extending the Policy Evaluation Toolkit",
    "section": "Bounds on the Average Treatment Effect",
    "text": "Bounds on the Average Treatment Effect\n\n\n\n\n\nFigure 12: Worst-Case Bounds on the Average Treatment Effect on the Treated, by Method"
  },
  {
    "objectID": "blog/drafts/partial-ID-panel/partial-ID-panel.html",
    "href": "blog/drafts/partial-ID-panel/partial-ID-panel.html",
    "title": "Bounds Under Endogenous Treatment Assignment: Panel Data Approaches",
    "section": "",
    "text": "Let \\(D=1\\) for treated and \\(D=0\\) for untreated.\nNeed at least three time periods (two pre and one post).\n\n\\(s\\) represents a generic time period\n\\(t\\), \\(t-1\\), and \\(t-2\\) represent the three time periods.\n\nPotential outcomes are given by \\(Y_{1s}\\) and \\(Y_{0s}\\).\n\\(Y_s\\) is the observed outcome in time period \\(s\\).\n\nCallaway:\n\nWilliamson and Downs (1990):\n\nFrandsen and Lefgren (2021):\n\n\n\n\n\n\n\n\\[\nATT = E[Y_{1t}-Y_{0t}|D=1]\n\\]\n\n\n\n\\[\nQTT(\\tau) = F_{Y_{1t|D=1}}^{-1}(\\tau) - F_{Y_{0t|D=1}}^{-1}(\\tau)\n\\]\n\n\n\n\\[\nDoTT(\\delta) = P(Y_{1t}-Y_{0t} \\leq \\delta | D=1)\n\\]\n\n\n\n\\[\nQoTT(\\tau) = \\inf \\{ \\delta: DoTT(\\delta) \\geq \\tau \\}\n\\] - QoTT(0.05) is the 5th percentile of the individual level treatment effect. - QoTT(0.5) is the median of the individual level treatment effect."
  },
  {
    "objectID": "blog/drafts/partial-ID-panel/partial-ID-panel.html#notation-and-quantities-of-interest",
    "href": "blog/drafts/partial-ID-panel/partial-ID-panel.html#notation-and-quantities-of-interest",
    "title": "Bounds Under Endogenous Treatment Assignment: Panel Data Approaches",
    "section": "",
    "text": "Let \\(D=1\\) for treated and \\(D=0\\) for untreated.\nNeed at least three time periods (two pre and one post).\n\n\\(s\\) represents a generic time period\n\\(t\\), \\(t-1\\), and \\(t-2\\) represent the three time periods.\n\nPotential outcomes are given by \\(Y_{1s}\\) and \\(Y_{0s}\\).\n\\(Y_s\\) is the observed outcome in time period \\(s\\).\n\nCallaway:\n\nWilliamson and Downs (1990):\n\nFrandsen and Lefgren (2021):\n\n\n\n\n\n\n\n\\[\nATT = E[Y_{1t}-Y_{0t}|D=1]\n\\]\n\n\n\n\\[\nQTT(\\tau) = F_{Y_{1t|D=1}}^{-1}(\\tau) - F_{Y_{0t|D=1}}^{-1}(\\tau)\n\\]\n\n\n\n\\[\nDoTT(\\delta) = P(Y_{1t}-Y_{0t} \\leq \\delta | D=1)\n\\]\n\n\n\n\\[\nQoTT(\\tau) = \\inf \\{ \\delta: DoTT(\\delta) \\geq \\tau \\}\n\\] - QoTT(0.05) is the 5th percentile of the individual level treatment effect. - QoTT(0.5) is the median of the individual level treatment effect."
  },
  {
    "objectID": "blog/drafts/partial-ID-panel/partial-ID-panel.html#hat-f_y_1ty_0t-1xd1",
    "href": "blog/drafts/partial-ID-panel/partial-ID-panel.html#hat-f_y_1ty_0t-1xd1",
    "title": "Bounds Under Endogenous Treatment Assignment: Panel Data Approaches",
    "section": "\\(\\hat F_{Y_{1t}|Y_{0t-1},X,D=1}\\)",
    "text": "\\(\\hat F_{Y_{1t}|Y_{0t-1},X,D=1}\\)\nWe will construct an estimate of \\(\\hat F_{Y_{1t}|Y_{0t-1},X,D=1}\\) using the observed post treatment period outcome among the treated group, but conditioning on the last pre treatment period outcome value.\n\\(\\hat F_{Y_{1t}|Y_{0t-1},X,D=1}\\) is identified through the sampling process—in much the way that \\(Y(1)|D=1\\) is an observed quantity as well (i.e., it does not involve a counterfactual). So in that sense, it’s the easiest place to start from.\nLet’s first collect the objects we need.\n\nY1t &lt;- # Outcome vector for treated group in post-treatment period\n  displacements %&gt;% filter(treat==1 & year==2011) %&gt;% pull(learn)\n\nY1tm1 &lt;-  # Outcome vector for treated group in last pre-treatment period. \n  # This is the variable we will condition on in the conditional CDF of Y1t.  \n  displacements %&gt;% filter(treat==1 & year==2007) %&gt;% pull(learn)\n\n# Get 100 evenly spaced points along the support of the outcome. \nYrange &lt;- \n  displacements %&gt;% pull(learn) %&gt;% range()\ny_ &lt;- \n  seq(floor(Yrange[1]),ceiling(Yrange[2]),length.out = 100)\n\nLet’s start with a “conditional” CDF that is just an intercept model—that is, a regression model that does not condition on the pre-treatment outcome. This seems a bit silly, as it should simply recover a bunch of identical CDFs. But it’s a good way to make sure the underlying machinery is working before we get into a more advanced model.\nHere is the process:\n\nIterate over each \\(X\\) variable value (i.e., \\(Y_{1tm1}\\), the treated group’s outcome in the last pre-treatment period). This is the “outer” loop.\nFor each value of X, iterate over the support of the outcome, i.e., y_. This is the “inner” loop.\n\nAt each value \\(y\\), construct an indicator (\\(IY\\)) set to one if \\(Y_{1t}&lt;=y\\) and 0 otherwise.\nRun the model of \\(IY=h(X)\\) we want. It could be a simple linear regression. It could be a nonparametric regression. Or some kind of flexible local logit regression (this is what the csabounds package uses). For now, it’s just an intercept model, i.e., lm(IY ~ 1).\n\n\nObtain the predicted values from this model.\n\n\n\nXvar &lt;- # X variable to condition on \n  Y1tm1[order(Y1tm1)]\n\nhatFY1 &lt;- # Empty object to collect the results\n  list()\n\nfor (.x in Xvar) { # outer loop is over X\n  res &lt;- list() # need to collect some results\n  for (.y in y_) { # inner loop is over y_ \n    IY &lt;-  # define a temporary (indicator) outcome.\n      as.integer(Y1t &lt;= .y)\n    X &lt;-   # The model matrix is simply an intercept\n      matrix(1, nrow = length(Y1t))\n    dat &lt;- # Combine together into a data frame. \n      cbind.data.frame(IY = IY, cons = X)\n    fit &lt;- # Fit a standard logit model of the binary outcome. \n      glm(IY ~ cons - 1, data = dat, family = \"binomial\")\n    res[[paste0(.y)]] &lt;-  # Obtain the predicted values of the model\n      predict(fit, type = \"response\") %&gt;% mean()\n  }\n  hatF &lt;- unlist(res)\n  \n  # We now have a vector of predicted values -- one for \n  # each value of y_. We will create a function that \n  # takes as its input y_, and outputs F(y_|X=.x) (i.e.,\n  # the predicted values from above.). \n  \n  hatFY1[[paste0(.x)]] &lt;- \n    approxfun(y_, hatF, method = \"constant\", yleft = 0, yright = 1, f = 0, ties = \"ordered\")\n}\n\nBecause this is just an intercept model that doesn’t actually condition on anything, it should simply recover the empirical CDF. Let’s verify this.\nThe below plots three objects:\n\nThe constructed “conditional” CDF for \\(X\\)=9.680344 [black]\nThe constructed “conditional” CDF for \\(X\\)=10.0774409 [red]\nThe calculated empirical CDF for \\(Y\\) (i.e., ecdf(Y1t)) [blue]\n\nFor 1-3, we randomly sample 30 points to plot (rather than show all 100 in y_) so that overlapping points can be viewed. As the figure makes clear, through an intercept-only model we have successfully recovered the (unconditional) empricial CDF.\n\n\n\n\n\nAnd for the sake of completeness, here’s the same but showing all 100 y_ points for each:\n\n\n\n\n\nLet’s next fit a generalized additive model that flexibly accommodates our \\(X\\) variable we want to condition on. The code below is nearly identical to the above, except rather than a linear model we fit gam(IY ~ s(Ytmin1), data = dat, family = \"binomial\").\nAgain, the plot below shows three CDFs:\n\nThe constructed “conditional” CDF for \\(X\\)=9.680344 [black]\nThe constructed “conditional” CDF for \\(X\\)=10.0774409 [red]\nThe calculated empirical CDF for \\(Y\\) (i.e., ecdf(Y1t)) [blue]\n\n\n\n\n\n\nThese are a bit too chunky, and they also violate the requirement that a CDF is monotonically increasing. We can address this by fitting a smooth function to the points, and imposing a shape constraint (i.e., monotonically increasing, between 0 and 1) using the methods here. This can be done using the R package scam, and using code inspired by this source.\n\n\n\n\n\nThat’s much better! And the net result is a new object called shatFY1 which has length 133–one conditional CDF for each of the unique values of Y1tm1 in the underlying data.\nIt’s worth noting that a generalized additive model is not the only choice we can make here. The csabounds package, for example, uses a local logistic regression. Let’s see how that compares. In the figure below, the blue points are based on a nonparametric GAM model, while the red points are based on local logistic regression. The estimated conditional CDFs are simliar, but not identical."
  },
  {
    "objectID": "blog/drafts/partial-ID-panel/partial-ID-panel.html#hat-f_y_0ty_0t-1xd1",
    "href": "blog/drafts/partial-ID-panel/partial-ID-panel.html#hat-f_y_0ty_0t-1xd1",
    "title": "Bounds Under Endogenous Treatment Assignment: Panel Data Approaches",
    "section": "\\(\\hat F_{Y_{0t}|Y_{0t-1},X,D=1}\\)",
    "text": "\\(\\hat F_{Y_{0t}|Y_{0t-1},X,D=1}\\)\nWe now need to fit the second preliminary object—and this object requires a counterfactual.\n\nChanges-in-Changes\nWe will appeal to a changes-in-changes identification framework to construct the counterfactual CDF.\nLet’s clearly state our target of interest right up front: what we are after is the conditional CDF of the treated group’s counterfactual outcomes in the post-treatment period.\nTo construct this counterfactual distribution, we’ll start in the last pre-treatment period (i.e., period \\(t-1\\)).\nFirst, let’s consider the distribution of the outcome of the untreated group at \\(t-1\\):\n\nY0tm1 &lt;- displacements %&gt;% filter(treat==0  & year==2007) %&gt;% pull(learn)\n\ndf_ &lt;-\n  tibble(y = Y0tm1)\n\np1 &lt;- df_ %&gt;% \n  ggplot(aes(x = y)) + geom_density() + \n  ggtitle(\"PDF of Y at t-1 in untreated group\")\n\np2 &lt;- df_ %&gt;% \n  ggplot(aes(x = y)) + stat_ecdf() + \n  ggtitle(\"eCDF of Y at t-1 in untreated group\")\n\np1 / p2\n\n\n\n\nOutcome Distribution of Untreated Group at t-1\n\n\n\n\nThe bottom panel of the figure shows the empirical CDF for this distribution. This function tells us, for a given value of \\(Y\\), the quantile in this distribution that value maps to.\nSo with this eCDF in mind, we can plug in the \\(Y\\) values for the treated group at the same time (t-1). That is, we can find the quantile in the untreated group’s distribution that each value of the treated group’s outcome maps to.\nIn the plots below, we overlay the treated group’s outcome at \\(t-1\\) in blue. The top panel shows that the distributions are somewhat different; the “rug” plot on the axes in bottom panel shows both where the density of values in the treated group are (x-axis), as well as what quantile in the untreated groups eCDF these values map to (y-axis).\n\np1_ &lt;- \n  df_ %&gt;% \n  ggplot(aes(x = y)) + geom_density() + \n  ggtitle(\"PDF of Y at t-1 in untreated (black)\\n and treated (blue) group\") + \n  geom_density(data = tibble(y = Y1tm1), col = \"blue\")\n\np2_ &lt;- \n  p2 + \n  geom_rug(data = tibble(x = Y1tm1,y=ecdf(Y0tm1)(Y1tm1)),aes(x=x,y=y), col = \"blue\") + \n  ggtitle(\"eCDFof Y at t-1 in untreated (black)\\n and treated (blue) group\") \np1_ / p2_\n\n\n\n\nOutcome Distribution of Untreated Group at t-1\n\n\n\n\nTo get the counterfactual distribution in the post period, we’ll take the quantile values from the y-axis of the bottom plot of the figure and ask: what happens at these quantiles in the post period for the untreated group?\nThis question makes the counterfactual intuition a bit more clear. Take, for example, a treated group indiviudal’s \\(t-1\\) outcome value of 10.79, which maps to (roughly) the 50th percentile of the untreated group’s (at time \\(t-1\\)) outcome distribution. We then ask, what is the value at the 50th percentile of the untreated group at time \\(t\\)?\n\nquantile(Y0t,0.5)\n\n     50% \n10.87805 \n\n\nSo here we see that, from the last pre period to the post period, the 50th percentile value changes from 10.79 to 10.88 in the untreated group. The value 10.88 is the counterfactual outcome in the post treatment period for a treated group individual with an outcome value of 10.79 in the last pre-treatment period.\n\nY1t_cfx &lt;-\n  quantile(Y0t, probs = ecdf(Y0tm1)(Y1tm1), type = 1)\n\nWe’ll now expand this exercise to all outcome values in the treated group to get a distribution of counterfactuals:\n\ncfx &lt;- \n  quantile(Y0t, probs = ecdf(Y0tm1)(Y1tm1), type = 1)\n\ndf_cfx &lt;-\n  df_ %&gt;% \n  mutate(y_cfx = cfx) %&gt;% \n  gather(measure, value)\n\np1 &lt;- df_cfx %&gt;% \n  ggplot(aes(x = value)) + geom_density(aes(colour = measure)) + \n  ggtitle(\"PDF of Counterfactual Outcome Distribution\")+ \n  theme(legend.position = \"top\") + \n  ggsci::scale_color_aaas(name=\"\")\n\np2 &lt;- df_cfx %&gt;% \n  ggplot(aes(x = value)) + stat_ecdf(aes(colour = measure)) + \n  ggtitle(\"eCDF of Counterfactual Outcome Distribution\") +\n  theme(legend.position = \"top\") + \n  ggsci::scale_color_aaas(name = \"\")\n\np1 / p2\n\n\n\n\nObserved and Counterfactual Outcome Distribution\n\n\n\n\nWe can create an eCDF function object based on the counterfactual distribution as follows:\n\nY1t_cfx &lt;-\n  quantile(Y0t, probs = ecdf(Y0tm1)(Y1tm1), type = 1)\n\nF.treated.t.cf &lt;- # CDF of counterfactual outcome distribution\n  ecdf(Y1t_cfx)\n\nF.treated.t.cf_ &lt;- ecdf(quantile(Y0t, probs = ecdf(Y0tm1)(Y1tm1), type = 1))\n\n\n\nEstimating the Conditional Counterfactual Distribution\nWe now have the necessary ingredients to estimate the conditional counterfactual distribution \\(\\hat F_{Y_{0t}|Y_{0t-1},X,D=1}\\). Our process will proceed as before, with some modifications.\nFor \\(\\hat F_{Y_{0t}|Y_{0t-1},X,D=1}\\), we directly observed how the outcome in the treated group changed from \\(t-1\\) to \\(t\\). That is, we directly observe the joint distribution \\((Y_{1t},Y_{1t-1})|D=1\\). But we don’t have that luxury now. So we have to invoke the copula stability assumption, which essentially says that we can learn about the joint distribution \\((Y_{0t},Y_{0t-1})|D=1\\) from the joint distribution \\((Y_{0t-1},Y_{0t-2})|D=1\\).\nAs before we’ll iterate over values of Y1tm1. But each time we consider a value, we’ll map it to its quantile and then find the value in the \\(t-2\\) outcome distribution associated with this quantile.\nFor example, let’s consider the value 10.5453414. This corresponds to the 0.3759398 quantile of its distribution. And the value of the Y1tm2 distribution at this quantile is 12.4909997.\n\n\n\n\n\n\n\nEstimating the Conditional CDF Using the Counterfactual"
  },
  {
    "objectID": "blog/drafts/partial-ID-panel/partial-ID-panel.html#construct-the-bounds",
    "href": "blog/drafts/partial-ID-panel/partial-ID-panel.html#construct-the-bounds",
    "title": "Bounds Under Endogenous Treatment Assignment: Panel Data Approaches",
    "section": "Construct the Bounds",
    "text": "Construct the Bounds"
  },
  {
    "objectID": "blog/drafts/partial-ID-panel/partial-ID-panel.html#fransen-and-lefgren",
    "href": "blog/drafts/partial-ID-panel/partial-ID-panel.html#fransen-and-lefgren",
    "title": "Bounds Under Endogenous Treatment Assignment: Panel Data Approaches",
    "section": "Fransen and Lefgren",
    "text": "Fransen and Lefgren"
  },
  {
    "objectID": "blog/drafts/partial-id-tools1/partial-id-tools1.html",
    "href": "blog/drafts/partial-id-tools1/partial-id-tools1.html",
    "title": "Buliding Blocks for Constructing Bounds: Part 1",
    "section": "",
    "text": "Introduction\n\n\nThis part will cover causal inference as a missing data issue.\nIt will also cover shortfalls of point identification\n\nStrong assumptions\nSensitivity to functional form\n\nPartial identification methods offer new objects of interest\n\nQuantities of interest not point identified, even in RCTs\n\nFraction of the population harmed or hurt by an intervention.\n\nNot as sensitive to functional form (e.g., parallel trends in logs or levels)?\n\n\nBasic structure can start with CI as a missing data problem.\n\nMissing information on potential outcomes.\nThis restricts the quantities of interest we can point identify, since we don’t observe the “coupling” of both potential outcomes.\n\nCan’t, for example, identify the fraction of the population harmed by an intervention.\n\nWe might estimate that the ATE is “good,” on average, but this doesn’t mean everyone is better off.\n\nDistribution of the treatment effect\n\nVarious identification methods bring assumptions that allow us to identify various quantities of interest.\n\nATET\n\n\n\n\nData Generation Process\n\n\nCode\nparams &lt;- \n  list(\n    N = 2e3,\n    sigma_sq_X = 1.0,\n    sigma_sq_epsilon = 0.3,\n    delta = 1\n  )\nparams &lt;- with(params,\n               modifyList(params,list(\n                 r_squared = 1 - sigma_sq_epsilon,\n                 beta = sqrt(1 - sigma_sq_epsilon),\n                 Sigma = matrix(c(sigma_sq_X,0,0,sigma_sq_epsilon),\n                                byrow=TRUE, nrow = 2, ncol = 2))))\n\ngen_data &lt;- function(params) {\n  with(params, \n       mvrnorm(n = N, mu = c(0,0), Sigma = Sigma)) %&gt;% \n    data.frame() %&gt;% \n    as_tibble() %&gt;% \n    set_names(c(\"X\",\"epsilon\")) %&gt;% \n    mutate(Y_i0 = params$beta * X + epsilon) %&gt;% \n    mutate(Y_i1 = Y_i0 + params$delta) %&gt;% \n    mutate(random = runif(nrow(.))) %&gt;% \n    mutate(D = as.integer(row_number()&lt;=(params$N)/2)) %&gt;% \n    arrange(random) %&gt;% \n    select(-random) %&gt;% \n    mutate(Y = D * Y_i1 + (1 - D) * Y_i0) %&gt;% \n    select(Y,D,X)\n}\n\nset.seed(123)\ndf &lt;- params %&gt;% gen_data()\n\ndf %&gt;% head(n=10) %&gt;% kable() %&gt;% \n    kable_styling()\n\n\n\n\nTable 1: First 10 rows of data\n\n\nY\nD\nX\n\n\n\n\n0.636525\n0\n1.136893\n\n\n-1.017545\n0\n-1.143835\n\n\n-2.660006\n0\n-1.769366\n\n\n-0.234232\n0\n-1.314321\n\n\n-0.552531\n1\n-1.236676\n\n\n-0.082499\n0\n-1.321069\n\n\n1.784970\n1\n1.516068\n\n\n0.771550\n1\n0.246692\n\n\n-0.690019\n0\n-1.433008\n\n\n-0.084229\n0\n-0.229395\n\n\n\n\n\n\n\n\n\n\nGenerating an Empirical Cumulative Distribution Function\nLet’s define some useful objects:\n\ny1 &lt;- # Vector of treated outcomes\n    df %&gt;% filter(D==1) %&gt;% pull(Y)\n\ny0 &lt;- # Vector of untreated outcomes\n    df %&gt;% filter(D==0) %&gt;% pull(Y)\n\ny_ &lt;- # Vector of the support of the outcome, with 100 evenly-spaced values along it. \n    seq(floor(min(y0,y1))-1,ceiling(max(y0,y1))+1, length.out = 100)\n\nOur first option is to construct by hand. This type of approach will come in handy later when we want to construct a conditional CDF.\n\n\nCode\neCDF_y1 &lt;- \n    y_ %&gt;% # Iterate over the support of y\n    map_dbl(~({\n        IY = as.integer(y1 &lt;= .x)\n        mean(IY)\n    }))\np0 &lt;-  # Plot the constructed eCDF\n    tibble(x = y_, y = eCDF_y1, method = \"Constructed\") %&gt;% \n    ggplot(aes(x = x, y = y)) + geom_step(lwd=2)\np0 \n\n\n\n\n\nFigure 1: Constructed Empirical Cumulative Distribution Function\n\n\n\n\nA second option is to simply use the R command ecdf(). This command creates a function that we can feed y values to to get their quantile in the distribution. Let’s overlay the plot with an eCDF constructed this way in yellow:\n\n\nCode\neCDF_y1_v2 &lt;- ecdf(y1)\ndf_v2 &lt;- tibble(x = y_, y = eCDF_y1_v2(y_), method = \"ecdf()\")\np0 + \n    geom_step(data = df_v2, col = \"yellow\")\n\n\n\n\n\nFigure 2: Constructed Empirical Cumulative Distribution Function\n\n\n\n\nFigure 1 and Figure 2 are both step functions—but occasionally we may need a smoothed version because the empirically-estimated version is a bit lumpy (e.g., it may not conform to the requirement that the cdf monotonically increases and is bounded by 0 and 1). We can address this by fitting a smooth function to the points, and imposing the necessary shape constraint for a CDF (i.e., monotonically increasing, between 0 and 1) using the methods here. This can be done using the R package scam, and using code inspired by this source.\n\n\nCode\ndat &lt;-\n    tibble(x = y_, cdf = ecdf(y1)(y_))\nn.knots = 20\nn &lt;- length(dat$x)\nfit &lt;-\n    scam::scam(cdf ~ s(x, bs = \"mpi\", k = n.knots),\n               data = dat,\n               weights = c(n, rep(1, n - 2), 10 * n))\n## interior knots\nxk &lt;- with(fit$smooth[[1]], knots[4:(length(knots) - 3)])\n## spline values at interior knots\nyk &lt;- predict(fit, newdata = data.frame(x = xk))\n## reparametrization into a monotone interpolation spline\nxg &lt;- seq(min(dat$x), max(dat$x), length = 100)\nf &lt;- stats::splinefun(xk, yk, \"hyman\")\ndat$cdf_sm = f(y_)\n\np0 + \n    geom_step(data = df_v2, col = \"yellow\") + \n    geom_line(data = dat, aes(y = cdf_sm), col = \"blue\",lwd=1.1)\n\n\n\n\n\nFigure 3: Constructed Empirical Cumulative Distribution Functions\n\n\n\n\n\n\nConditional CDFs\nFirst, the long way …\n\n\nCode\ni1 &lt;-  # index of treated observations\n  df %&gt;% \n  mutate(i = row_number()) %&gt;% \n  filter(D==1) %&gt;% \n  pull(i)\n\ni0 &lt;-  # index of untreated observations\n  df %&gt;% \n  mutate(i = row_number()) %&gt;% \n  filter(D==0) %&gt;% \n  pull(i)\n\nX1 &lt;- # X values of treated observations\n  df %&gt;%\n  filter(D==1) %&gt;%\n  pull(X)\n\nX0 &lt;- # X values of untreated observations\n  df %&gt;% \n  filter(D==0) %&gt;% \n  pull(X)\n\nlibrary(tictoc)\ntic()\nhatFX_ &lt;- \n    y0[c(25,50)] %&gt;% \n    map(~{\n        i &lt;- which(y0==.x); i\n        res_ &lt;- map_dbl(y0,~({\n            IY = as.integer(y0 &lt;= .x)\n            X &lt;- X0\n            df_ &lt;- data.frame(IY = IY , X = X)\n            fit &lt;- gam(IY ~ s(X), data = df_, family = \"binomial\")\n            predict(fit, newdata = data.frame(X = X0[i]), type = \"response\")\n        }))\n        BMisc::makeDist(y0,res_)\n    })\ntime1 &lt;- toc()\n\n\n75.796 sec elapsed\n\n\nCode\np1 &lt;- \n  tibble(y = c(y0,y0), cdf = c(hatFX_[[1]](y0),hatFX_[[2]](y0)),id = c(rep(\"A\",length(y0)),rep(\"B\",length(y0)))) %&gt;% \n  ggplot(aes(x = y, y = cdf, group = id)) +\n  geom_step(lwd=1.5)\n\np1\n\n\n\n\n\nFigure 4: Conditional Empirical Cumulative Distribution Function\n\n\n\n\n\n\\[\nF_{Y_{j} \\mid X_{j}}(y \\mid x) \\equiv \\int_{(0,1)} 1\\left\\{Q_{Y_{j} \\mid X_{j}}(u \\mid x) \\leq y\\right\\} d u\n\\]\n\n\\[\n\\hat{F}_{Y_{j} \\mid X_{j}}(y \\mid x)=\\varepsilon+\\int_{(\\varepsilon, 1-\\varepsilon)} 1\\left\\{x^{\\prime} \\hat{\\beta}_{j}(u) \\leq y\\right\\} d u\n\\]\n\n\\[\n\\hat{\\beta}_{j}(u)=\\arg \\min _{b \\in \\mathbb{R}^{d x}} \\sum_{i=1}^{n_{j}}\\left[u-1\\left\\{Y_{j i} \\leq X_{j i}^{\\prime} b\\right\\}\\right]\\left[Y_{j i}-X_{j i}^{\\prime} b\\right]\n\\]\n\n\n0.455 sec elapsed"
  },
  {
    "objectID": "blog/resources/csabounds/NEWS.html",
    "href": "blog/resources/csabounds/NEWS.html",
    "title": "csabounds 1.0.0",
    "section": "",
    "text": "csabounds 1.0.0\n\nThe first version of the package csabounds is available\nAdded methods csabounds (the main method in the package)\nmethod attcpo (for computing the average treatment effect conditional on the previous outcome\nmethods ggcsabounds and ggattcpo for plotting the results"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nThe Vanderbilt Center for Health Economic Modeling was launched in 2020 to harness the diverse, internationally-recognized expertise of Vanderbilt researchers in modeling the health and economic consequences of health policy and technology changes worldwide.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelfare Analysis Meets Decision Theory\n\n\n\nCEA\n\n\nMVPF\n\n\n\n\n\n\n\nJohn Graves\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelfare Analysis Meets Research Prioritization\n\n\n\nCEA\n\n\nMVPF\n\n\n\n\n\n\n\nJohn Graves\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelfare Analysis Meets Study Design\n\n\n\nCEA\n\n\nMVPF\n\n\n\n\n\n\n\nJohn Graves\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]